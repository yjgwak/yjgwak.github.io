---
title: Fri, 23 Jun 2023
date: 2023-06-23
---
1. 9.5 [Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning](https://arxiv.org/abs/2306.12964)
* Authors: Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, Qing He
* Reason: The authors of this paper concentrate on the prioritizing the mining of a synergistic set of alphas. Their RL framework has shown effectiveness and efficiency in real-world stock market data forecasting. This could have a significant impact because of the widespread application in financial markets.

2. 9.3 [Decentralized Multi-Agent Reinforcement Learning with Global State Prediction](https://arxiv.org/abs/2306.12926)
* Authors: Joshua Bloom, Pranjal Paliwal, Apratim Mukherjee, Carlo Pinciroli
* Reason: This paper approaches a significant challenge in robot swarm control using multi-agent DRL. Instead of global information it uses Global State Prediction (GSP). The researchers demonstrated that including GSP enhances the performance and robustness compared with traditional, globally-informed, approach.

3. 9.1 [Sum-Rate Maximization of RSMA-based Aerial Communications with Energy Harvesting: A Reinforcement Learning Approach](https://arxiv.org/abs/2306.12977)
* Authors: Jaehyup Seong, Mesut Toka, Wonjae Shin
* Reason: In this research, a beneficial problem associated with aerial communications is solved using DRL. Novel power and beamforming design has been used for maximizing the sum-rate via RSMA. The proposed scheme has shown superiority over several conventional methods in terms of average sum-rate performance.

4. 8.9 [State-wise Constrained Policy Optimization](https://arxiv.org/abs/2306.12594)
* Authors: Weiye Zhao, Rui Chen, Yifan Sun, Tianhao Wei, Changliu Liu
* Reason: Presents an innovative and thorough exploration regarding reinforcement learning challenges in real-world situations. The paper tackles safety concerns in reinforcement learning and offers practical solutions such as state-wise constraints. Authored by prominent figures in the field.

5. 8.9 [Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping](https://arxiv.org/abs/2306.12981)
* Authors: Yining Li, Peizhong Ju, Ness Shroff
* Reason: This paper offers a solution to the common RL problem of exponential growth of states and actions in high-dimensional spaces. The authors leverage the inherent structure of the problem to reduce sample/computational complexity.

6. 8.7 [Reinforcement Federated Learning Method Based on Adaptive OPTICS Clustering](https://arxiv.org/abs/2306.12859)
* Authors: Tianyu Zhao, Junping Du, Yingxia Shao, Zeli Guan
* Reason: This paper handles an important issue associated with federated learning, that is the impact caused by the non-independent and identical distribution of data across different user terminals. It suggests an innovative solution using adaptive OPTICS clustering that enhances the performance of federated learning tasks.

7. 8.6 [MP3: Movement Primitive-Based (Re-)Planning Policy](https://arxiv.org/abs/2306.12729)
* Authors: Fabian Otto, Hongyi Zhou, Onur Celik, Ge Li, Rudolf Lioutikov, Gerhard Neumann
* Reason: The authors present a comprehensive approach integrating movement primitives into the deep RL framework. This unique approach can potentially drive future research in the reinforcement learning community.

8. 8.2 [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery](https://arxiv.org/abs/2306.12802)
* Authors: Hoang Thanh Lam, Marco Luca Sbodio, Marcos Martínez Gallindo, Mykhaylo Zayats, Raúl Fernández-Díaz, Víctor Valls, Gabriele Picco, Cesar Berrospi Ramis, Vanessa López
* Reason: Although not specific to reinforcement learning, this paper offers valuable insights on representation learning techniques, which are fundamental in developing robust RL models. The benchmarks provided by the authors can potentially help in the development and validation of reinforcement learning techniques in healthcare.

9. 7.9 [Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning](https://arxiv.org/abs/2306.12755)
* Authors: Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang, Sibo Gai, Donglin Wang
* Reason: The paper explores cross-domain offline reinforcement learning and introduces new techniques to tackle data inefficiency, a significant challenge in RL. However, the authors are not as well-established as in other papers.

10. 7.5 [Improving Long-Horizon Imitation Through Instruction Prediction](https://arxiv.org/abs/2306.12554)
* Authors: Joey Hejna, Pieter Abbeel, Lerrel Pinto
* Reason: The paper introduces an innovative approach in improving the performance of planning environments in reinforcement learning. However, its potential influence is moderate as its focus on using language as auxiliary supervision represents a relatively niche area of the broad field of reinforcement learning.

