---
title: Tue, 17 Oct 2023
date: 2023-10-17
---
1. 9.6 [Alpha Elimination: Using Deep Reinforcement Learning to Reduce Fill-In during Sparse Matrix Decomposition](https://arxiv.org/abs/2310.09852)
* Authors: Arpan Dasgupta, Pawan Kumar
* Abstract: A reinforcement learning based approach is proposed to tackle a challenge in matrix decomposition, specifically the NP-hard problem of finding an optimal reordering algorithm that leads to minimal fill-in. The proposed method significantly reduces non-zeros in the LU-decomposition compared to existing heuristic algorithms, demonstrating the potential impact of reinforcement learning in computational and scientific applications.

2. 9.6 [In-Context Pretraining: Language Modeling Beyond Document Boundaries](https://arxiv.org/abs/2310.10638)
* Authors: Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis
* Reason: The paper introduces a novel approach - In-Context Pretraining - which trains language models on related document sequences. The proposed method has shown notable improvements in numerous tasks that require in-depth context analysis.

3. 9.3 [Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games](https://arxiv.org/abs/2310.09727)
* Authors: Youbang Sun, Tao Liu, Ruida Zhou, P. R. Kumar, Shahin Shahrampour
* Abstract: This paper examines an independent natural policy gradient (NPG) algorithm for MARL in Markov potential games. The researchers prove that under certain conditions, this method can reach an ε-Nash Equilibrium within O(1/ε) iterations, an improvement on previous results. This study provides implications for improving the speed and effectiveness of reinforcement learning methods, particularly in multi-agent reinforcement learning scenarios.

4. 9.3 [Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models](https://arxiv.org/abs/2310.10635)
* Authors: Thomas Decker, Ananta R. Bhattarai, Michael Lebacher
* Reason: This work utilizes deep generative models for scenario-based validation of autonomous train's safety, which is a crucial and complex task. The method demonstrates utility for more representative testing from limited data, which could have a significant impact on the field.

5. 9.2 [Efficacy of Dual-Encoders for Extreme Multi-Label Classification](https://arxiv.org/abs/2310.10636)
* Authors: Nilesh Gupta, Devvrit Khatri, Ankit S Rawat, Srinadh Bhojanapalli, Prateek Jain, Inderjit S Dhillon
* Reason: This paper explores dual-encoder models' effectiveness in Extreme Multi-Label Classification (XMC) tasks and suggests a differentiable topk error-based loss function. Given the complexity of XMC, their findings could significantly influence the development of future models.

6. 9.1 [MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization](https://arxiv.org/abs/2310.09833)
* Authors: Simin Li, Ruixiao Xu, Jun Guo, Pu Feng, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu, Weifeng Lv
* Abstract: The researchers propose a novel approach MIR2 for robust multi-agent reinforcement learning. MIR2, which trains in routine scenarios, minimizes mutual information as a form of robust regularization, achieving better resilience against worst-case adversaries than max-min optimization coding under experimental conditions.

7. 9.1 [AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents](https://arxiv.org/abs/2310.09971)
* Authors: Jake Grigsby, Linxi Fan, Yuke Zhu
* Reason: Paper introduces a scalable in-context Reinforcement Learning (RL) agent, AMAGO, applicable to a wide range of problems and demonstrates empirically in meta-RL and long-term memory domains. Its focus on sparse rewards and off-policy data can potentially expand the RL research field.

8. 9.1 [Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems](https://arxiv.org/abs/2310.10603)
* Authors: Chendi Qian, Didier Chételat, Christopher Morris
* Reason: The authors successfully demonstrate how Graph Neural Networks (GNNs) can simulate standard interior-point methods for Linear Optimization Problems. The insights drawn from this work could significantly enhance the understanding and utility of GNNs in optimization problems.

9. 9.0 [Hybrid Reinforcement Learning for Optimizing Pump Sustainability in Real-World Water Distribution Networks](https://arxiv.org/abs/2310.09412)
* Authors: Harsh Patel, Yuan Zhou, Alexander P Lamb, Shu Wang, Jieliang Luo
* Abstract: This paper addresses the water distribution networks optimization problem with an improved "hybrid RL" methodology, combining reinforcement learning adaptability with operational data as a foundation for the agent's actions. The findings show significant improvement in sustainability, operational efficiency, and adaptability to emerging scenarios in real-world WDNs.

10. 9.0 [ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots](https://arxiv.org/abs/2310.10486)
* Authors: Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert
* Reason: This work focuses on the development of a single locomotion policy that is capable of controlling a variety of quadruped robots. This research could be highly influential due to its implications in the field of robotics, specifically in reducing the complexity related to learning processes of diverse robot types.

11. 8.9 [Dynamic Link Prediction for New Nodes in Temporal Graph Networks](https://arxiv.org/abs/2310.09787)
* Authors: Xiaobo Zhu, Yan Wu, Qinhu Zhang, Zhanheng Chen, Ying He
* Abstract: This paper presents a novel model that performs dynamic link prediction on new nodes by considering the problem as few-shot. Using a temporal encoder with node-level span memory to get nodes embedding and leveraging meta-learning, the model performs better than existing state-of-the-art methods in various tests on publicly available datasets.

12. 8.9 [Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels](https://arxiv.org/abs/2310.09997)
* Authors: Thomas Jiralerspong, Flemming Kondrup, Doina Precup, Khimya Khetarpal
* Reason: The paper introduces Forecaster, a deep hierarchical reinforcement learning approach beneficial in complex environments from high-dimensional state space. It provides a practical strategy to leverage long-term planning and could influence model efficiency and problem-solving scale in RL.

13. 8.8 [Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance](https://arxiv.org/abs/2310.10021)
* Authors: Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, Joseph J. Lim
* Reason: The paper introduces BOSS, an approach to solve new long-horizon tasks by growing a learned skill library. This provides a new perspective and promising methodology in RL for task learning with minimal supervision.

14. 8.6 [Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning](https://arxiv.org/abs/2310.10103)
* Authors: Dhruv Shah, Michael Equi, Blazej Osinski, Fei Xia, Brian Ichter, Sergey Levine
* Reason: The paper brings a new approach to navigation in unfamiliar environments using "semantic guesswork" produced by language models, demonstrating the power of integrating language models into RL for more effective decision-making.

15. 8.4 [Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation](https://arxiv.org/abs/2310.10250)
* Authors: Simon Hakenes, Tobias Glasmachers
* Reason: The paper presents a novel method of using topological maps for deep reinforcement learning in multi-object navigation. This could open a fresh path for solving complex RL problems in expansive spaces.

