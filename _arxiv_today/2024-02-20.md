---
title: Tue, 20 Feb 2024
date: 2024-02-20
---
1. 9.5 [Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks](https://arxiv.org/abs/2402.12067)
* Authors: Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Laurenz Wiskott
* Reason: Presents a neuroscientifically inspired approach to RL with potential broad impact on RL visual navigation tasks, published in a workshop at AAAI which is a top conference in AI.

2. 9.2 [Revisiting Data Augmentation in Deep Reinforcement Learning](https://arxiv.org/abs/2402.12181)
* Authors: Jianshu Hu, Yunpeng Jiang, Paul Weng
* Reason: Accepted to ICLR 2024, a premier conference on learning representations, offering insights into data augmentation for DRL, including a novel regularization term.

3. 9.0 [Refining Minimax Regret for Unsupervised Environment Design](https://arxiv.org/abs/2402.12284)
* Authors: Michael Beukman, Samuel Coward, Michael Matthews, Mattie Fellows, Minqi Jiang, Michael Dennis, Jakob Foerster
* Reason: Innovative concept (minimax regret refinement) presented in RL context with potential implications for unsupervised learning environments, indicating a strong base in RL strategy.

4. 8.9 [Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics](https://arxiv.org/abs/2402.11317)
* Authors: Xinyu Zhang, Wenjie Qiu, Yi-Chen Li, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu
* Reason: Introduces an innovative approach to a significant challenge in RL, relating to offline learning and policy adaptability in changing environments. The paper is likely to be influential due to its practical approach (DORA) in solving these issues and the fact that it addresses a fundamental bottleneck in offline RL scenarios.

5. 8.9 [CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation](https://arxiv.org/abs/2402.12222)
* Authors: Jueon Eom, Seyeon Jeong, Taekyoung Kwon
* Reason: Effective combination of RL with fuzzing for software testing, demonstrating notable empirical results, which could have significant implications for software security testing.

6. 8.7 [Self-evolving Autoencoder Embedded Q-Network](https://arxiv.org/abs/2402.11604)
* Authors: J. Senthilnath, Bangjian Zhou, Zhen Wei Ng, Deeksha Aggarwal, Rajdeep Dutta, Ji Wei Yoon, Aye Phyu Phyu Aung, Keyu Wu, Min Wu, Xiaoli Li
* Reason: Proposes an original RL approach (SAQN) which combines a self-evolving autoencoder with a Q-Network, and demonstrates state-of-the-art performance on benchmark environments. This paper is likely influential due to its novel architecture that enhances the exploration capability of RL agents.

7. 8.7 [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models](https://arxiv.org/abs/2402.12336)
* Authors: Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein
* Reason: Addresses a critical need for robustness in multi-modal models especially regarding the vision component, with practical implications for improving security against adversarial attacks.

8. 8.5 [Reinforcement learning to maximise wind turbine energy generation](https://arxiv.org/abs/2402.11384)
* Authors: Daniel Soler, Oscar Mariño, David Huergo, Martín de Frutos, Esteban Ferrer
* Reason: Presents a real-world application of reinforcement learning, showcasing superior performance of RL strategies over traditional PID controls in the domain of wind turbine energy generation. Its practical contribution to a significant area of renewable energy suggests potential high impact.

9. 8.3 [Multi Task Inverse Reinforcement Learning for Common Sense Reward](https://arxiv.org/abs/2402.11367)
* Authors: Neta Glazer, Aviv Navon, Aviv Shamsian, Ethan Fetaya
* Reason: Tackles the complex issue of reward hacking and alignment in reinforcement learning by proposing multi-task inverse reinforcement learning to derive common-sense reward functions. The paper could influence future research in domain-agnostic RL tasks and ethical AI development.

10. 8.1 [Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics](https://arxiv.org/abs/2402.11515)
* Authors: Wang Jia, Hang Xu
* Reason: Addresses the challenge of computational cost in training DRL models for active flow control, which has broad implications across various fields in engineering and physical sciences. The potential impact lies in bringing efficiency to training processes and scalability of DRL applications.

