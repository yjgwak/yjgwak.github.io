---
title: Mon, 10 Jul 2023
date: 2023-07-10
---
1. 9.8 [Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning](https://arxiv.org/abs/2307.03406)
* Authors: Zilai Zeng, Ce Zhang, Shijie Wang, Chen Sun
* Reason: This paper presents a two-stage framework for summarizing trajectories with sequence modeling techniques and then using these for learning policies for decision making. Their model provides impressive empirical performance in several environments, demonstrating the power of sequence modeling in decision-making tasks.

2. 9.6 [Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning](https://arxiv.org/abs/2307.03486)
* Authors: Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song
* Reason: The authors propose a new contrastive learning method that significantly enhances the agent's capability to predict future achievements. Their proposed method demonstrates a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on challenging environments with fewer resources.

3. 9.4 [Online Network Source Optimization with Graph-Kernel MAB](https://arxiv.org/abs/2307.03641)
* Authors: Laura Toni, Pascal Frossard
* Reason: The work presents an innovative combination of Bayesian and frequentist principles to learn the optimal source placement in online networks. It provides theoretical performance guarantees and practical empirical results outperforming existing offline methods.

4. 9.2 [SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation](https://arxiv.org/abs/2307.03716)
* Authors: Cameron Berg, Vittorio Caggiano, Vikash Kumar
* Reason: Leveraging an evolutionary motor control strategy, the authors use physiologically accurate human hand and leg models to determine if a Synergistic Action Representation eases learning of complex tasks. Their model shows impressive performance and shows zero-shot generalization to out-of-domain environmental conditions.

5. 9.1 [Polybot: Training One Policy Across Robots While Embracing Variability](https://arxiv.org/abs/2307.03719)
* Authors: Jonathan Yang, Dorsa Sadigh, Chelsea Finn
* Reason: This paper presents a novel method of training a single policy over multiple robotic platforms, a significant step towards scaling vision-based robotic manipulators. Their approach shows significant improvement in success rate when using new task data collected on a different robot, demonstrating the utility of their proposed design.

