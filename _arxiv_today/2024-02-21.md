---
title: Wed, 21 Feb 2024
date: 2024-02-21
---
1. 9.1 [The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2402.12527)
* Authors: Anya Sims, Cong Lu, Yee Whye Teh
* Reason: Authors with leading authority in the field, compelling evidence of a new foundational issue in model-based reinforcement learning, and a novel solution proposed. Likely to influence subsequent research.

2. 8.9 [Skill or Luck? Return Decomposition via Advantage Functions](https://arxiv.org/abs/2402.12874)
* Authors: Hsiao-Ru Pan, Bernhard Sch√∂lkopf
* Reason: The paper is authored by researchers with strong reputations, addresses a key problem of off-policy learning in RL, and provides an innovative decomposition method, attracting interest in the RL community.

3. 8.6 [Offline Multi-task Transfer RL with Representational Penalization](https://arxiv.org/abs/2402.12570)
* Authors: Avinandan Bose, Simon Shaolei Du, Maryam Fazel
* Reason: Strong author team and addresses an important issue in offline RL with a unique approach to representation transfer, making it potentially influential.

4. 8.2 [Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies](https://arxiv.org/abs/2402.12673)
* Authors: Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang
* Reason: Notable contribution to robustness in RL with innovative adaptive defense mechanism, spotlight presentation at a major conference increases its potential impact.

5. 7.7 [In deep reinforcement learning, a pruned network is a good network](https://arxiv.org/abs/2402.12479)
* Authors: Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro
* Reason: The paper offers practical improvements to network efficiency in deep RL with potential for broad application, although the influence may be more incremental in nature.

