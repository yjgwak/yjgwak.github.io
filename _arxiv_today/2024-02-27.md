---
title: Tue, 27 Feb 2024
date: 2024-02-27
---
1. 9.3 [Feedback Efficient Online Fine-Tuning of Diffusion Models](https://arxiv.org/abs/2402.16359)
* Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso Biancalani
* Reason: Sergey Levine is a known authority in the field of reinforcement learning, contributing to the influence score. The paper integrates reinforcement learning with diffusion models, offering a novel approach and applicable theory with potentially broad impact on complex data domains.

2. 9.2 [How Can LLM Guide RL? A Value-Based Approach](https://arxiv.org/abs/2402.16181)
* Authors: Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang
* Reason: Integrates large language models (LLMs) with reinforcement learning (RL), a novel combination that could potentially enhance RL algorithms' sample efficiency. The practical approach outlined and the successful demonstration across diverse environments make this a potentially influential paper. The author list includes researchers from established institutes, likely to have high authority in the field.

3. 9.1 [Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications](https://arxiv.org/abs/2402.15650)
* Authors: Zihan Zhou, Jonathan Booher, Wei Liu, Aleksandr Petiushko, Animesh Garg
* Reason: This paper presents a novel method for safely managing multiple constraints in reinforcement learning which is critical for domains like autonomous driving. Given the practical implications and author Animesh Garg’s authority in the field, this paper shows potential high influence.

4. 9.1 [DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning](https://arxiv.org/abs/2402.15957)
* Authors: Anthony Liang, Guy Tennenholtz, Chih-wei Hsu, Yinlam Chow, Erdem Bıyık, Craig Boutilier
* Reason: Introduces a significant advancement in meta-reinforcement learning with innovative modifications to benefit various environments. The authorship includes Craig Boutilier, a well-respected figure in the field, which increases the credibility and potential impact of the paper.

5. 9.0 [Model-based deep reinforcement learning for accelerated learning from flow simulations](https://arxiv.org/abs/2402.16543)
* Authors: Andre Weiner, Janis Geise
* Reason: Model-based reinforcement learning is critical for many real-world applications, including flow simulations which have significant practical implications. This paper demonstrates substantial computational savings, indicating notable potential influence.

6. 8.9 [HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding](https://arxiv.org/abs/2402.15546)
* Authors: Huijie Tang, Federico Berto, Zihan Ma, Chuanbo Hua, Kyuree Ahn, Jinkyoo Park
* Reason: Accepted at AAMAS 2024, this paper addresses scalability challenges in multi-agent systems using Heuristics-Informed Multi-Agent Pathfinding (HiMAP), which could have a substantial impact on areas requiring large-scale coordination.

7. 8.9 [Graph Diffusion Policy Optimization](https://arxiv.org/abs/2402.16302)
* Authors: Yijing Liu, Chao Du, Tianyu Pang, Chongxuan Li, Wei Chen, Min Lin
* Reason: Presents a new method for optimizing graph diffusion models using reinforcement learning, which is a novel approach with potential for significant impact in generating graphs for complex objectives. Empirical results supporting state-of-the-art performance add to the potential influence of this paper.

8. 8.8 [Combining Transformer based Deep Reinforcement Learning with Black-Litterman Model for Portfolio Optimization](https://arxiv.org/abs/2402.16609)
* Authors: Ruoyu Sun, Angelos Stefanidis, Zhengyong Jiang, Jionglong Su
* Reason: Portfolio optimization is a high-impact area in finance, and this paper’s novel approach of integrating DRL with the Black-Litterman model shows promise in performance improvement, indicating a high potential for influence in financial applications.

9. 8.7 [Foundation Policies with Hilbert Representations](https://arxiv.org/abs/2402.15567)
* Authors: Seohong Park, Tobias Kreiman, Sergey Levine
* Reason: Authored by Sergey Levine, a well-known figure in reinforcement learning, the paper introduces a novel approach for pre-training generalist policies, presenting a significant step in unsupervised reinforcement learning.

10. 8.7 [Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning](https://arxiv.org/abs/2402.15893)
* Authors: Lunet Yifru, Ali Baheri
* Reason: Addresses the critical issue of safety in reinforcement learning, proposing an approach for concurrent learning of policy and safety constraints. The relevance of safety and adaptability in real-world applications heightens the importance of this work.

11. 8.6 [Behavioral Refinement via Interpolant-based Policy Diffusion](https://arxiv.org/abs/2402.16075)
* Authors: Kaiqi Chen, Eugene Lim, Kelvin Lin, Yiyang Chen, Harold Soh
* Reason: Offers advancement in imitation learning through the concept of interpolant-based policy diffusion. Considering the authors' credibility and the integration of theoretical and empirical contributions, this paper stands out for its innovative approach to a relevant problem in imitation learning.

12. 8.5 [Truly No-Regret Learning in Constrained MDPs](https://arxiv.org/abs/2402.15776)
* Authors: Adrian Müller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao He
* Reason: The paper confronts a vital challenge in reinforcement learning with CMDPs, providing safer learning algorithms that guarantee safety during the learning process. The authors’ backgrounds suggest this work will be influential in promoting secure learning environments.

13. 8.5 [Q-FOX Learning: Breaking Tradition in Reinforcement Learning](https://arxiv.org/abs/2402.16562)
* Authors: Mahmood Alqaseer, Yossra H. Ali, Tarik A. Rashid
* Reason: Proposes an innovative technique for hyperparameter tuning in RL, which is a critical aspect of model performance. The paper presents strong empirical results, suggesting it could be influential for researchers looking to optimize RL algorithms.

14. 8.3 [Teacher-Student Learning on Complexity in Intelligent Routing](https://arxiv.org/abs/2402.15665)
* Authors: Shu-Ting Pi, Michael Yang, Yuying Zhu, Qun Liu
* Reason: Addressing e-commerce customer service, an important commercial domain, this paper presents a machine learning framework to improve customer routing efficiency. The inclusion in the KDD 2023 Workshop signifies its relevance and potential influence in applied machine learning.

15. 8.2 [Language-guided Skill Learning with Temporal Variational Inference](https://arxiv.org/abs/2402.16354)
* Authors: Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris, Nicolas Le Roux, Marc-Alexandre Côté, Xingdi Yuan
* Reason: Bridging natural language processing with reinforcement learning, this research could substantially contribute to the development of more intuitive human-robot interactions, though the specificity to skill learning may slightly narrow its scope of influence.

