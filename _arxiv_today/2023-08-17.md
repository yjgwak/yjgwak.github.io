---
title: Thu, 17 Aug 2023
date: 2023-08-17
---
1. 9.5 [InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models](https://arxiv.org/abs/2308.08500)
* Authors: Kabir Nagrecha, Lingyi Liu, Pablo Delgado, Prasanna Padmanabhan
* Reason: This paper is highly relevant to the field of machine learning and reinforcement learning, especially in the area of data pipeline optimization and recommendation systems. It presents InTune, a new solution using a reinforcement learning agent capable of improving data ingestion rates, which has seen practical applications in Netflix's compute cluster.

2. 9.3 [A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction](https://arxiv.org/abs/2308.08502)
* Authors: Karan Gadgil, Sukhpal Singh Gill, Ahmed M. Abdelmoniem
* Reason: This paper focuses on meta-learning for predicting customer lifetime value, a crucial aspect in many business domains. It proposes a novel model using stacked regression techniques which could impact business decisions related to customer relationship management and marketing.

3. 9.2 [ResBuilder: Automated Learning of Depth with Residual Structures](https://arxiv.org/abs/2308.08504)
* Authors: Julian Burghoff, Matthias Rottmann, Jill von Conta, Sebastian Schoenen, Andreas Witte, Hanno Gottschalk
* Reason: This paper devises a neural architecture search algorithm, Resbuilder, for developing ResNet architectures, which is crucial in the deep learning community. The resulting models show close to state-of-the-art performance while saving computational costs.

4. 9.1 [A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes](https://arxiv.org/abs/2308.08069)
* Authors: Akhilesh Raj, Swann Perarnau, Aniruddha Gokhale
* The paper explores the use of Reinforcement Learning (RL) for reducing the power consumption in data centers without affecting application performance. The approach employs Proximal Policy Optimization (PPO) algorithms in balancing power and performance which makes it a potentially impactful research in the field of energy-efficient computing.

5. 9.1 [Can Transformers Learn Optimal Filtering for Unknown Systems?](https://arxiv.org/abs/2308.08536)
* Authors: Haldun Balim, Zhe Du, Samet Oymak, Necmiye Ozay
* Reason: This paper extends transformers' application beyond NLP tasks and explores their potential for problems in dynamical systems. It discusses a meta-output-predictor (MOP) that learns and quickly adapts to different systems, achieving comparable performance to Kalman filter-based optimal output estimators without access to a model.

6. 9.0 [CDR: Conservative Doubly Robust Learning for Debiased Recommendation](https://arxiv.org/abs/2308.08461)
* Authors: ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang
* Reason: This paper tackles the problem of bias in recommendation systems using a Conservative Doubly Robust strategy to reduce poisonous imputations. The proposed method offers reduced variance and improved tail risk, leading to potentially impactful improvements on recommender system accuracy.

7. 8.9 [Active Inverse Learning in Stackelberg Trajectory Games](https://arxiv.org/abs/2308.08017)
* Authors: Yue Yu, Jacob Levy, Negar Mehr, David Fridovich-Keil, Ufuk Topcu
* This work proposes an active inverse learning method to infer a player's objective function in Stackelberg trajectory games. The approach outperforms uniform random inputs in terms of convergence speed, making it significant for the development of efficient learning algorithms in game-theoretic settings.

8. 8.8 [Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning](https://arxiv.org/abs/2308.08029)
* Authors: Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith
* This paper presents an extension to the sophisticated inference algorithm that incorporates active learning during planning. It balances directed exploration and counterfactual reasoning which could enrich the current reinforcement learning landscape with new methods for dynamic problem solving.

9. 8.6 [Ada-QPacknet: adaptive pruning with bit width reduction as an efficient continual learning method without forgetting](https://arxiv.org/abs/2308.07939)
* Authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo
* The paper describes a new architecture-based CL method Ada-QPacknet which applies pruning and quantization techniques for improving learning efficiency. With its remarkable performance in comparison with other established algorithms, it could contribute to the evolution of efficient learning methods in dynamic environments.

10. 8.4 [Probabilistic Black-Box Checking via Active MDP Learning](https://arxiv.org/abs/2308.07930)
* Authors: Junya Shijubo, Masaki Waga, Kohei Suenaga
* In this work, a novel approach, ProbBBC, has been introduced for testing stochastic black-box systems. By integrating techniques like probabilistic model checking and statistical hypothesis testing, it offers a promising approach for enhancing system verification processes.

