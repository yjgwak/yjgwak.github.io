---
title: Thu, 28 Sep 2023
date: 2023-09-28
---
1. 9.6 [Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need](https://arxiv.org/abs/2309.15737)
* Authors: Danil Provodin, Pratik Gajane, Mykola Pechenizkiy, Maurits Kaptein
* Reason: The paper presents an algorithm for infinite-horizon undiscounted learning in constrained Markov decision processes (CMDP) with promising theoretical and empirical results.

2. 9.5 [Evaluating Cognitive Maps and Planning in Large Language Models with CogEval](https://arxiv.org/abs/2309.15129)
* Authors: Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, Jonathan Larson
* Reason: The paper offers a novel and systematic evaluation for cognitive capabilities in Large Language Models. The work is from reputable authors and offers a unique insight into cognitive maps and the planning abilities of these models.

3. 9.4 [Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization](https://arxiv.org/abs/2309.15603)
* Authors: Bang Giang Le, Viet Cuong Ta
* Reason: The paper opens a new direction in multi-task reinforcement learning by replacing the Kullback-Leibler divergence with a novel optimal transport-based regularization.

4. 9.3 [From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency](https://arxiv.org/abs/2309.15133)
* Authors: Ling Cheng, Feida Zhu, Yong Wang, Ruicheng Liang, Huiwen Liu
* Reason: This paper tackles a very current and potentially significant issue of malicious activities in the Cryptocurrency world. The authors propose an application-specific machine learning model which could have an enormous impact in detecting illegal activities earlier.

5. 9.2 [Jointly Training Large Autoregressive Multimodal Models](https://arxiv.org/abs/2309.15564)
* Authors: Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, Barlas Oguz
* Reason: The paper introduces JAM framework which is designed to fuse text and image generation models for mixed-modal generation tasks.

6. 9.1 [Deep Generative Methods for Producing Forecast Trajectories in Power Systems](https://arxiv.org/abs/2309.15137)
* Authors: Nathan Weill, Jonathan Dumas
* Reason: With the growing importance of renewable energy, understanding and forecasting the functioning of power systems is crucial. This paper devises deep learning models to generate energy production and load forecast trajectories. It stands to reason that such a tool could massively impact current power systems.

7. 9.0 [MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis](https://arxiv.org/abs/2309.15521)
* Authors: Angelo Yamachui Sitcheu, Nils Friederich, Simon Baeuerle, Oliver Neumann1, Markus Reischl, Ralf Mikut
* Reason: The paper explores the complete application of MLOps in the context of scarce data analysis, and proposes a new approach in biomedical image analysis.

8. 8.9 [Conservative World Models](https://arxiv.org/abs/2309.15178)
* Authors: Scott Jeen, Tom Bewley, Jonathan M. Cullen
* Reason: This paper discusses the concept of zero-shot reinforcement learning and how it can create a more efficient model when trained on smaller datasets. This approach could significantly improve the efficiency of learning algorithms.

9. 8.7 [STARC: A General Framework For Quantifying Differences Between Reward Functions](https://arxiv.org/abs/2309.15257)
* Authors: Joar Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, Alessandro Abate
* Reason: A technical paper that presents a new method for quantifying the difference between reward functions in reinforcement learning. The methodology proposed could provide a more principled framework for developing reward learning algorithms.

10. 8.7 [Startup success prediction and VC portfolio simulation using CrunchBase data](https://arxiv.org/abs/2309.15552)
* Authors: Mark Potanin, Andrey Chertok, Konstantin Zorin, Cyril Shtabtsovsky
* Reason: The paper presents an novel deep learning model for predicting startup success and offers a robust evaluation of the model's performance against historical data.

