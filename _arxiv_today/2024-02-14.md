---
title: Wed, 14 Feb 2024
date: 2024-02-14
---
1. 9.5 [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://arxiv.org/abs/2402.08609)
* Authors: Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, Pablo Samuel Castro
* Reason: Proposes a method that enables RL models to scale with more parameters, potentially prompting a breakthrough in performance. The authors include established researchers like Doina Precup and Pablo Samuel Castro, promising rigorous research and influence in the field.

2. 9.3 [World Model on Million-Length Video And Language With RingAttention](https://arxiv.org/abs/2402.08268)
* Authors: Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel
* Reason: High-profile authors such as Pieter Abbeel, and the topic's relevance to large-scale language and video models suggest potential influence on multimodal reinforcement learning and long-form tasks.

3. 9.2 [Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins](https://arxiv.org/abs/2402.08421)
* Authors: Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, Hirley Alves
* Reason: This paper tackles the challenge of applying RL in the offline setting, which is crucial for practical applications like digital twins, indicating a potential industry impact.

4. 9.0 [Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea](https://arxiv.org/abs/2402.08502)
* Authors: Hanna Krasowski, Matthias Althoff
* Reason: Offering a safe RL approach with proven compliance to traffic regulations has strong implications for the deployment of autonomous vehicles and maritime systems.

5. 8.9 [SMX: Sequential Monte Carlo Planning for Expert Iteration](https://arxiv.org/abs/2402.07963)
* Authors: Matthew V Macfarlane, Edan Toledo, Donal Byrne, Siddarth Singh, Paul Duckworth, Alexandre Laterre
* Reason: This paper presents SMX, a significant advancement in scalable planning algorithms for reinforcement learning. The introduction of a model-based planning algorithm with high parallelisation potential and applicability to both discrete and continuous action spaces, alongside demonstrating statistical improvement over influential works like AlphaZero, positions this work as highly influential.

6. 8.7 [A Competition Winning Deep Reinforcement Learning Agent in microRTS](https://arxiv.org/abs/2402.08112)
* Authors: Scott Goodfriend
* Reason: The paper showcases the first Deep Reinforcement Learning agent to win at the IEEE microRTS competition, overcoming limitations faced by DRL algorithms in RTS games. The practical implications for future competitions and research in reinforcement learning and their successful strategies make this work notably influential.

7. 8.7 [A Distributional Analogue to the Successor Representation](https://arxiv.org/abs/2402.08530)
* Authors: Harley Wiltzer, Jesse Farebrother, Arthur Gretton, Yunhao Tang, Andr√© Barreto, Will Dabney, Marc G. Bellemare, Mark Rowland
* Reason: The introduction of a distributional successor measure for RL with connections to both model-based learning and distributional RL approaches could influence how uncertainty is considered in policy evaluations.

8. 8.3 [Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks](https://arxiv.org/abs/2402.08022)
* Authors: Talha Bozkus, Urbashi Mitra
* Reason: This work presents a novel ensemble Q-learning algorithm optimized for complex wireless networks. It achieves significant performance improvements over state-of-the-art reinforcement learning algorithms, backed by convergence analyses, making it a relevant contribution to the field.

9. 8.1 [Avoiding Catastrophe in Continuous Spaces by Asking for Help](https://arxiv.org/abs/2402.08062)
* Authors: Benjamin Plaut, Hanlin Zhu, Stuart Russell
* Reason: The paper addresses a critical issue in reinforcement learning related to irreparable mistakes by proposing a new variant of the contextual bandit problem. Its importance score is driven by the inclusion of a mentor guidance mechanism and steps towards reducing catastrophic outcomes, concepts that can be profoundly influential in the broader scope of safe reinforcement learning.

10. 7.9 [Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation](https://arxiv.org/abs/2402.08184)
* Authors: Ayesha Siddika Nipu, Siming Liu, Anthony Harris
* Reason: The paper proposes a framework for transfer learning in Multi-Agent Reinforcement Learning, which is a significant challenge in the field. The research demonstrates improvements in various scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, potentially influencing the approach to learning in MAS.

