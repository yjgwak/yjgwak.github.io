---
title: Fri, 29 Sep 2023
date: 2023-09-29
---
1. 9.5 [Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning](https://arxiv.org/abs/2309.16074)
* Authors: Feiyang Wu, Zhaoyuan Gu, Hanran Wu, Anqi Wu, Ye Zhao
* Reason: Authors introduce novel algorithms for learning expert reward functions in the challenging area of bipedal locomotion over complex terrains, which extends previous work and brings new insights into robotics and locomotion.

2. 9.3 [Task-Oriented Koopman-Based Control with Contrastive Encoder](https://arxiv.org/abs/2309.16077)
* Authors: Xubo Lyu, Hanyang Hu, Seth Siriya, Ye Pu, Mo Chen
* Reason: The paper presents an end-to-end reinforcement learning method with contrastive encoders for learning on high-dimensional, complex nonlinear systems, which innovatively extends the use of Kooman control, a widely known approach in reinforcement learning.

3. 9.1 [Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned](https://arxiv.org/abs/2309.16291)
* Authors: Brieuc Pinon, RaphaÃ«l Jungers, Jean-Charles Delvenne
* Reason: The paper delves into fundamental limitations of common Reinforcement Learning methods, which makes a theoretical contribution to the field, potentially driving future research into more efficient alternatives.

4. 9.0 [CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture](https://arxiv.org/abs/2309.16299)
* Authors: Zixuan Chen, Ze Ji, Shuyang Liu, Jing Huo, Yiyu Chen, Yang Gao
* Reason: The proposed framework innovatively uses human cognitive priors and introduces a novel dual cognition-action architecture, offering a new perspective in the problem of imitation learning in robotics, a popular topic in reinforcement learning.

5. 9.0 [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks](https://arxiv.org/abs/2309.16347)
* Authors: Eleftherios Triantafyllidis, Filippos Christianos, Zhibin Li
* Reason: Approach to enhance exploration in reinforcement learning. Notable performance over related methods.

6. 8.8 [Stackelberg Batch Policy Learning](https://arxiv.org/abs/2309.16188)
* Authors: Wenzhuo Zhou, Annie Qu
* Reason: The paper presents a novel approach to policy learning, interpreting the learning process as a two-player general-sum game which extends previous approaches in reinforcement learning and offers new insights into the conceptual understanding of the task.

7. 8.7 [RLLTE: Long-Term Evolution Project of Reinforcement Learning](https://arxiv.org/abs/2309.16382)
* Authors: Mingqi Yuan, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, Wenjun Zeng
* Reason: Presents a comprehensive framework for RL research and application, useful for academics and industry professionals.

8. 8.5 [Uncertainty-Aware Decision Transformer for Stochastic Driving Environments](https://arxiv.org/abs/2309.16397)
* Authors: Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao
* Reason: Introduces an uncertainty-aware Transformer for offline RL, demonstrating superior performance in various driving scenarios.

9. 8.3 [MotionLM: Multi-Agent Motion Forecasting as Language Modeling](https://arxiv.org/abs/2309.16534)
* Authors: Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, Benjamin Sapp
* Reason: Proposes an innovative approach to motion forecasting using language modeling, achieving state-of-the-art performance.

10. 8.1 [Robust Offline Reinforcement Learning -- Certify the Confidence Interval](https://arxiv.org/abs/2309.16631)
* Authors: Jiarui Yao, Simon Shaolei Du
* Reason: Develops an algorithm to certify the robustness of a given policy offline, important for the security of RL.

