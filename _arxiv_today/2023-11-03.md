---
title: Fri, 3 Nov 2023
date: 2023-11-03
---
1. 9.7 [DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing](https://arxiv.org/abs/2311.01450)
* Authors: Vint Lee, Pieter Abbeel, Youngwoon Lee
* Reason: Authors have established reputations in the field of reinforcement learning, and the paper addresses a significant challenge in this field: handling sparse rewards. The novel method of reward smoothing has the potential to significantly improve the effectiveness of MBRL algorithms.

2. 9.6 [Learning to Design and Use Tools for Robotic Manipulation](https://arxiv.org/abs/2311.00754)
* Authors: Ziang Liu, Stephen Tian, Michelle Guo, C. Karen Liu, Jiajun Wu
* Reason: Accepted at a prestigious conference (CoRL 2023). Authors have prior influential works. Unique approach to use reinforcement learning to allow rapid prototyping of specialized tools for robotic manipulation tasks.

3. 9.6 [Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability](https://arxiv.org/abs/2311.01406)
* Authors: Stefan Kambiz Behfar, Jon Crowcroft
* Reason: The introduced concept combines Graph Attention Network and Reinforcement Learning to analyze and optimize Ethereum Network efficiency, which is very current and impactful. Also, Crowcroft is a well-known authority in computer science.

4. 9.4 [RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation](https://arxiv.org/abs/2311.01455)
* Authors: Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan
* Reason: The paper explores a novel idea of generative simulation for robotic learning, which could lead to the acceleration of advanced robotic capabilities. Multiple of the authors have a strong track record and influence in the field.

5. 9.2 [Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2311.00865)
* Authors: Matthias Gerstgrasser, Tom Danino, Sarah Keren
* Reason: Published at NeurIPS 2023. Paper offers a novel approach to multi-agent reinforcement learning that permits decentralized training - potentially a significant contribution to the field.

6. 9.2 [Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time](https://arxiv.org/abs/2311.01435)
* Authors: Xinyuan Cao, Santosh S. Vempala
* Reason: Importance of this paper mostly comes by its potential on paving the way to new methods of halfspace learning using symmetric one-dimensional logconcave distributions. It presents a polynomial-time algorithm for an important problem in unsupervised learning, with a novel use of contrastive moments.

7. 9.1 [Diffusion Models for Reinforcement Learning: A Survey](https://arxiv.org/abs/2311.01223)
* Authors: Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, Weinan Zhang
* Reason: Notable authors from Shanghai Jiao Tong University, and the paper offers a targeted survey on the application of diffusion models in reinforcement learning (RL), a hot topic in modern ML research.

8. 9.1 [Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation](https://arxiv.org/abs/2311.01446)
* Authors: Jay Sarva, Jingkang Wang, James Tu, Yuwen Xiong, Sivabalan Manivasagam, Raquel Urtasun
* Reason: This work addresses a critical issue in the safety of self-driving vehicles by proposing a new method for testing their performance in challenging scenarios. While not exclusively on reinforcement learning, the methodology and its implications could influence the direction of autonomous vehicle research and development.

9. 9.0 [SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org/abs/2311.00880)
* Authors: Jaafar Mhamed, Shangding Gu
* Reason: This paper introduces a novel method for tackling an important issue in RL: safety. The proposed solution, SCPO, offers balance between maximizing rewards and adhering to safety constraints, an aspect crucial for real-world RL applications.

10. 8.9 [Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2311.01075)
* Authors: Siming Lan, Rui Zhang, Qi Yi, Jiaming Guo, Shaohui Peng, Yunkai Gao, Fan Wu, Ruizhi Chen, Zidong Du, Xing Hu, Xishan Zhang, Ling Li, Yunji Chen
* Reason: Accepted at NeurIPS. It introduces a novel method for multi-task RL that combines contrastive learning with temporal attention, addressing some key issues with existing approaches.

11. 8.8 [Learning Realistic Traffic Agents in Closed-loop](https://arxiv.org/abs/2311.01394)
* Authors: Chris Zhang, James Tu, Lunjun Zhang, Kelvin Wong, Simon Suo, Raquel Urtasun
* Reason: Real-world application in developing self-driving software. Paper introduces a novel, holistic approach for matching expert demonstrations under traffic compliance constraints.

12. 8.7 [Dynamic Fair Federated Learning Based on Reinforcement Learning](https://arxiv.org/abs/2311.00959)
* Authors: Weikang Chen, Junping Du, Yingxia Shao, Jia Wang, Yangxi Zhou
* Reason: The paper addresses a highly relevant problem in federated learning - fairness. Utilizing reinforcement learning for dynamic parameter tuning, this approach might lead to significant improvements in federated learning performance.

13. 8.6 [A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories](https://arxiv.org/abs/2311.01329)
* Authors: Kai Yan, Alexander G. Schwing, Yu-Xiong Wang
* Reason: Offer safe and efficient solutions for offline imitation from observations, especially when incomplete trajectories are presented.

14. 8.5 [Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching](https://arxiv.org/abs/2311.01331)
* Authors: Kai Yan, Alexander G. Schwing, Yu-xiong Wang
* Reason: Directly tackling some of the challenges with the state-of-the-art DICE methods for offline learning from observations, further developing the field.

15. 8.3 [Effective Human-AI Teams via Learned Natural Language Rules and Onboarding](https://arxiv.org/abs/2311.01007)
* Authors: Hussein Mozannar, Jimin J Lee, Dennis Wei, Prasanna Sattigeri, Subhro Das, David Sontag
* Reason: Selected as Spotlight at NeurIPS 2023, the paper focuses on improving human-AI collaboration via learned rules and an onboarding stage. It's a practical approach to a rapidly emerging issue in AI and promises significant improvements to human-AI team performance.

