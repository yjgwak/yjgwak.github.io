---
title: Wed, 26 Jul 2023
date: 2023-07-26
---
1. 9.9 [Settling the Sample Complexity of Online Reinforcement Learning](https://arxiv.org/abs/2307.13586)
* Authors: Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du
* Reason: The paper addresses a fundamental problem in online reinforcement learning and introduces a new method that achieves minimax-optimal regret without any burn-in cost. The approach could potentially revolutionize how reinforcement learning algorithms are designed and evaluated.

2. 9.5 [Contrastive Example-Based Control](https://arxiv.org/abs/2307.13101)
* Authors: Kyle Hatch, Benjamin Eysenbach, Rafael Rafailov, Tianhe Yu, Ruslan Salakhutdinov, Sergey Levine, Chelsea Finn
* Reason: The authors propose a new way of handling real-world reinforcement learning problems, which could lead to more robust and scalable RL algorithms.

3. 9.2 [Counterfactual Explanation Policies in RL](https://arxiv.org/abs/2307.13192)
* Authors: Shripad V. Deshmukh, Srivatsan R, Supriti Vijay, Jayakumar Subramanian, Chirag Agarwal
* Reason: The paper tackles the important issue of explainability in reinforcement learning, opening the door for more reliable and accountable AI systems in decision-making roles.

4. 8.9 [Deep Reinforcement Learning for Robust Goal-Based Wealth Management](https://arxiv.org/abs/2307.13501)
* Authors: Tessa Bauman, Bruno Gašperov, Stjepan Begušić, Zvonko Kostanjčar
* Reason: The paper brings reinforcement learning techniques to financial applications, which could impact and potentially revolutionize wealth management practices.

5. 8.7 [Unbiased Weight Maximization](https://arxiv.org/abs/2307.13270)
* Authors: Stephen Chung
* Reason: This paper introduces Unbiased Weight Maximization, providing a novel and theoretically grounded learning rule that can potentially speed up training in neural networks that emulate stochastic reinforcement learning agents.

