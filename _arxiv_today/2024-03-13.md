---
title: Wed, 13 Mar 2024
date: 2024-03-13
---
1. 9.4 [Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/abs/2403.07652)
* Authors: Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng
* Reason: This paper offers a novel approach to enhancing efficiency in MoE models by dynamically adjusting the number of experts, potentially impacting a broad array of applications and contributing to the critical discourse on computational resource allocation in complex tasks.

2. 8.9 [Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines](https://arxiv.org/abs/2403.07005)
* Authors: Xuejing Zheng, Chao Yu
* Reason: The paper introduces a new approach to cooperative Multi-Agent Reinforcement Learning (MARL) which is a key area of research within reinforcement learning. The use of Reward Machines (RMs) for task decomposition could be an influential advancement in managing complex environments, suggesting a high potential impact in the field.

3. 8.9 [Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning](https://arxiv.org/abs/2403.07704)
* Authors: Motoki Omura, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada
* Reason: The paper addresses a fundamental issue in reinforcement learning - the skewness of Bellman error. By proposing a method that creates a normal error distribution, it advances the field of deep reinforcement learning and has potential for significance in sample efficiency improvements.

4. 8.7 [Ant Colony Sampling with GFlowNets for Combinatorial Optimization](https://arxiv.org/abs/2403.07041)
* Authors: Minsu Kim, Sanghyeok Choi, Jiwoo Son, Hyeonah Kim, Jinkyoo Park, Yoshua Bengio
* Reason: Inclusion of Yoshua Bengio, a well-known authority in machine learning, and the novel integration of Generative Flow Networks (GFlowNets) with ant colony optimization indicate that this could significantly influence future research in combinatorial optimizations in RL.

5. 8.7 [Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding](https://arxiv.org/abs/2403.07559)
* Authors: Huijie Tang, Federico Berto, Jinkyoo Park
* Reason: The authors present a new method in the domain of MARL-MAPF, showing competitive performance against state-of-the-art methods. The approach addresses significant challenges in communication-based multi-agent environments and could substantially influence the efficiency of coordinated multi-agent systems.

6. 8.5 [Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation](https://arxiv.org/abs/2403.07261)
* Authors: Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu, Lei Yuan, Zongzhang Zhang, Yang Yu
* Reason: The paper tackles a major challenge in Offline Meta-Reinforcement Learning (OMRL) with a novel adversarial data augmentation approach. Given the complexity and importance of offline learning, this contribution could be quite influential.

7. 8.5 [CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability](https://arxiv.org/abs/2403.07632)
* Authors: Gregory W. Kyro, Matthew T. Martin, Eric D. Watt, Victor S. Batista
* Reason: This work intersects machine learning and drug discovery with a focus on reducing drug-induced cardiotoxicity risks. The framework can potentially influence the drug development pipeline and personalized medicine, leading to safer healthcare solutions.

8. 8.2 [Advantage-Aware Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2403.07262)
* Authors: Yunpeng Qing, Shunyu liu, Jingyuan Cong, Kaixuan Chen, Yihe Zhou, Mingli Song
* Reason: This research addresses a pertinent problem in Offline RL with a new method of advantage-aware policy constraints. As Offline RL continues to attract attention, this method may prove highly beneficial.

9. 8.2 [Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning](https://arxiv.org/abs/2403.07611)
* Authors: Vinay Chakravarthi Gogineni, Esmaeil S. Nadimi
* Reason: The paper introduces innovative algorithms for machine unlearning, which is becoming increasingly important due to data privacy regulations. The proposed methods could have significant implications for model efficacy and compliance with data protection policies.

10. 8.0 [Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer](https://arxiv.org/abs/2403.07309)
* Authors: Dipesh Tamboli, Jiayu Chen, Kiran Pranesh Jotheeswaran, Denny Yu, Vaneet Aggarwal
* Reason: The focus on a critical healthcare application like sepsis treatment and its claimed improvement over existing methods signify potential real-world impact, which is substantial for reinforcement learning research.

