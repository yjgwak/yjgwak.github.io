---
title: Tue, 22 Aug 2023
date: 2023-08-22
---
1. 9.7 [Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes](https://arxiv.org/abs/2308.09733)
* Authors: Sherif Abdelfattah, Kathryn Merrick, Jiankun Hu
* This paper proposes a novel dual-phase intrinsically motivated reinforcement learning method to address multi-objective Markov decision processes. Its potential impact is high because of its significant performance enhancement over state-of-the-art methods and its application potential in dynamic robotics environments.

2. 9.7 [Federated Learning Robust to Byzantine Attacks: Achieving Zero Optimality Gap](https://arxiv.org/abs/2308.10427)
* Authors: Shiyuan Zuo, Rongfei Fan, Han Hu, Ning Zhang, Shimin Gong
* Reason: The authors are experts in the field, and the topic of a robust aggregation method for federated learning can provide fundamental influence on the developments in federated learning space. The paper's approach in overcoming the issue of Byzantine attacks, assuring zero optimality gap, sets a high standard for subsequent research.

3. 9.5 [Structured World Models from Human Videos](https://arxiv.org/abs/2308.10901)
* Authors: Russell Mendonca, Shikhar Bahl, Deepak Pathak
* Reason: The paper proposes a novel approach to enabling robots to efficiently learn complex behaviors using human video data. The novelty of the approach and wide applicability in complex real-world scenarios combined with the credibility of the authors' backgrounds indicates a high potential for influence.

4. 9.4 [A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments](https://arxiv.org/abs/2308.09734)
* Authors: Sherif Abdelfattah, Kathryn Kasmarik, Jiankun Hu
* The paper introduces a developmental optimization approach for evolving policy in non-stationary environments, which makes it important considering the non-stationarity nature of real-world applications.

5. 9.3 [Adaptive Local Steps Federated Learning with Differential Privacy Driven by Convergence Analysis](https://arxiv.org/abs/2308.10457)
* Authors: Xinpeng Ling, Jie Fu, Zhili Chen
* Reason: The paper provides a detailed analysis on the convergence of federated learning with differential privacy on resource-constrained scenarios and proposes a new algorithm. This topic is critical in securing privacy in the era of big data and machine learning.

6. 9.1 [A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks](https://arxiv.org/abs/2308.10664)
* Authors:  Nikolaos Koursioumpas, Lina Magoula, Nikolaos Petropouleas, Alexandros-Ioannis Thanopoulos, Theodora Panagea, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili
* Reason: This research tackles an important problem in the intersection of AI and wireless networks and shows promising results. The approach has broad applications and will likely impact both academia and the industry.

7. 9.0 [Skill Transformer: A Monolithic Policy for Mobile Manipulation](https://arxiv.org/abs/2308.09873)
* Authors: Xiaoyu Huang, Dhruv Batra, Akshara Rai, Andrew Szot
* This paper offers a flow for long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. It shows a high success rate on benchmarked tasks which can directly translate to practical utilities in robotics.

8. 9.0 [Deep Learning of Delay-Compensated Backstepping for Reaction-Diffusion PDEs](https://arxiv.org/abs/2308.10501)
* Authors: Shanshan Wang, Mamadou Diagne, Miroslav Krstić
* Reason: The authors are able to expand their framework to the approximation of multiple nonlinear operators. They therefore contribute significantly to the field of control of PDE systems with their DeepONet-approximated nonlinear operator and provide rigorous theoretical results with promising simulations.

9. 8.8 [Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees](https://arxiv.org/abs/2308.10487)
* Authors: Lue Tao, Yu-Xuan Huang, Wang-Zhou Dai, Yuan Jiang
* Reason: This work proves significant for its comprehensive theoretical analysis on neuro-symbolic hybrid systems and provides a groundbreaking method to determine the efficacy of knowledge in learning. The contribution to the understanding of hyper system's learnability cannot be overemphasized.

10. 8.7 [DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2308.09902)
* Authors: Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, Shuai Li
* The paper presents the DPMAC algorithm which protects the sensitive information of individual agents. Considering ongoing discussions on privacy protection, this paper could be influential in setting up privacy standards in multi-agent reinforcement learning.

11. 8.7 [CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making](https://arxiv.org/abs/2308.10707)
* Authors: Giovanni Minelli, Mirco Musolesi
* Reason: The paper presents a new framework for multi-agent systems, enabling efficient decentralised coordination and independent decision making. Given the growing importance of multi-agent systems, this new approach could be influential in the field.

12. 8.5 [Never Explore Repeatedly in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2308.09909)
* Authors: Chenghao Li, Tonghan Wang, Chongjie Zhang, Qianchuan Zhao
* This paper proposes a dynamic reward scaling approach to address the revisitation issue in the exploration process of multi-agent reinforcement learning. It is potentially influential due to its remarkable performance enhancement in demanding environments.

13. 8.5 [Towards Accelerated Model Training via Bayesian Data Selection](https://arxiv.org/abs/2308.10544)
* Authors: Zhijie Deng, Peng Cui, Jun Zhu
* Reason: A conceptually new method for handling mislabeled, duplicated, or biased data for model training is introduced in this paper. The method builds on a Bayesian treatment and zero-shot predictors, which has shown significant gains in training efficiency compared to other baselines.

14. 8.3 [Stabilizing Unsupervised Environment Design with a Learned Adversary](https://arxiv.org/abs/2308.10797)
* Authors: Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, Tim Rocktäschel
* Reason: Researching on Unsupervised Environment Design (UED), a key challenge in agent training, the paper proposes solutions for shortcomings of current UED approaches, which is a significant contribution to the field.

15. 8.0 [Reinforcement Learning Based Sensor Optimization for Bio-markers](https://arxiv.org/abs/2308.10649)
* Authors: Sajal Khandelwal, Pawan Kumar, Syed Azeemuddin
* Reason: The findings of this research could have a significant impact on fields that rely on radio frequency biosensors. The approach is novel and the paper shows improvements over state-of-the-art methods in enhancing sensor sensitivity.

