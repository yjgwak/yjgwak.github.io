---
title: Tue, 12 Sep 2023
date: 2023-09-12
---
1. 9.5 [Compositional Learning of Visually-Grounded Concepts Using Reinforcement](https://arxiv.org/abs/2309.04504)
* Authors: Zijun Lin, Haidi Azaman, M Ganesh Kumar, Cheston Tan
* Reason: The authors provide a comprehensive exploration of reinforcement learning concepts, illustrated with multiple real-world examples. The paper explores the potential for compositional learning and demonstrates significant advancements in training efficiency and generalization capabilities.

2. 9.5 [Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning](https://arxiv.org/abs/2309.05582)
* Authors: Marin Vlastelica, Sebastian Blaes, Cristina Pineri, Georg Martius
* Reason: The paper proposes an innovative method for managing risk in model-based reinforcement learning that involves probabilistic safety constraints and balancing of optimism/pessimism in the face of various uncertainties. The separation of uncertainties is proven to be crucial for performance in uncertain and safety-critical control environments.

3. 9.3 [Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2309.04615)
* Authors: Zhizun Wang, David Meger
* Reason: This paper presents a novel model-based multi-agent reinforcement learning approach, addressing crucial challenges in the field. The authors employ variable autoencoders and present these in real-world scenarios, showcasing superior performance compared to other baselines.

4. 9.2 [A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications](https://arxiv.org/abs/2309.05343)
* Authors: Wei Wang, Peizheng Li, Angela Doufexi, Mark A Beach
* Reason: The paper introduces a novel deep reinforcement learning (DRL)-based optimization method for reflection enhancement in the context of wireless communication systems. The method outperforms existing alternatives and achieves significant improvements without requiring hardware modifications.

5. 9.1 [Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing](https://arxiv.org/abs/2309.04612)
* Authors: Wangyang Ying, Dongjie Wang, Kunpeng Liu, Leilei Sun, Yanjie Fu
* Reason: The paper presents a unique approach to feature generation, discussing meaningful ways to generate features and the challenges of learning tasks for machines. The proposed framework seems effective and flexible in tackling different learning tasks, which could be influential in the field.

6. 9.1 [Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs](https://arxiv.org/abs/2309.04831)
* Authors: Xiangyuan Zhang, Saviz Mowlavi, Mouhacine Benosman, Tamer Ba≈üar
* Reason: Authors have proven track records, topic is innovative and stimulates further research in reinforcement learning field. Also, the RHPG algorithm is globally convergent, which addresses a limitation in reinforcement learning applications.

7. 8.9 [Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation](https://arxiv.org/abs/2309.04655)
* Authors: Jinwoo Lee, Kangkyu Kwon, Ira Soltis, Jared Matthews, Yoonjae Lee, Hojoong Kim, Lissette Romero, Nathan Zavanelli, Youngjin Kwon, Shinjae Kwon, Jimin Lee, Yewon Na, Sung Hoon Lee, Ki Jun Yu, Minoru Shinohara, Frank L. Hammond
* Reason: The paper presents a novel approach to exoskeleton systems using cloud-based deep learning to predict human intention, potentially making significant advancements in the field of robotics and AI-based augmented systems.

8. 8.9 [Continual Robot Learning using Self-Supervised Task Inference](https://arxiv.org/abs/2309.04974)
* Authors: Muhammad Burhan Hafez, Stefan Wermter
* Reason: Paper provides important investigation into the ongoing problem in robot learning of mastering single tasks; novel self-supervised task inference can have wide application.

9. 8.8 [Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach](https://arxiv.org/abs/2309.05391)
* Authors: Spyros Avlonitis, Dor Lavi, Masoud Mansoury, David Graus
* Reason: This paper tackles a real-world problem of career planning using a reinforcement learning approach. It leverages algorithms such as Sarsa, Q-Learning, and A2C, providing strategies to optimally maximize long-term income.

10. 8.7 [Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective](https://arxiv.org/abs/2309.04707)
* Authors: Muzhe Guo, Feixu Yu, Tian Lan, Fang Jin
* Reason: The authors offer a novel approach to deep reinforcement learning models, making them more interpretable. The paper analyses agents' decision-making process in novel ways, contributing to more responsible and trustworthy reinforcement learning.

11. 8.7 [Convex Q Learning in a Stochastic Environment: Extended Version](https://arxiv.org/abs/2309.05105)
* Authors: Fan Lu, Sean Meyn
* Reason: Introduces first-ever convex Q-learning formulation which may have significant influence in reinforcement learning field; the paper also gives thorough theoretical justifications and practical implications.

12. 8.5 [CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization](https://arxiv.org/abs/2309.05200)
* Authors: Yang Xu, Ronghao Zheng, Senlin Zhang, Meiqin Liu, Shoudong Huang
* Reason: Presents a new hybrid method for efficiently exploring unknown and complex environments, and have provided open-source code, which may be very useful for practitioners and researchers in robotics and reinforcement learning.

13. 8.5 [Physics-informed reinforcement learning via probabilistic co-adjustment functions](https://arxiv.org/abs/2309.05404)
* Authors: Nat Wannawas, A. Aldo Faisal
* Reason: The research introduces two novel approaches to more accurately quantify the dynamics of real systems like a biomechanical human arm. This approach enhances the efficiency and uncertainty-awareness of reinforcement learning methods, resulting in more accurate controls.

14. 8.3 [Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout](https://arxiv.org/abs/2309.05213)
* Authors: Pengfei Guo, Warren Richard Morningstar, Raviteja Vemulapalli, Karan Singhal, Vishal M. Patel, Philip Andrew Mansfield
* Reason: Offers practical solutions for federated learning under constrained resources, which is a pressing issue in the machine learning community. Their technique may be highly influential in resource-constrained domains.

15. 8.0 [Robot Parkour Learning](https://arxiv.org/abs/2309.05665)
* Authors: Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao
* Reason: The research addresses the challenge of legged locomotion in complex, variable environments. With the use of a novel reinforcement learning approach, robots are trained to develop parkour skills that are effectively transferable to real-world operations.

