---
title: Mon, 28 Aug 2023
date: 2023-08-28
---
1. 9.9 [Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory](https://arxiv.org/abs/2308.13011)
* Authors: Karthik Somayaji NS, Yu Wang, Malachi Schram, Jan Drgona, Mahantesh Halappanavar, Frank Liu, Peng Li
* Reason: This paper presented a new approach to enhance the resilience of RL agents. It provides substantial theoretical justification and showed successful practical evaluations, making it potentially impactful.

2. 9.5 [Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car](https://arxiv.org/abs/2308.13088)
* Authors: Aakaash Salvaji, Harry Taylor, David Valencia, Trevor Gee, Henry Williams
* Reason: The paper focuses on applying Deep Reinforcement Learning for end-to-end control of an autonomous race car - a popular and complex application. It's accepted at the Australasian Conference on Robotics and Automation (ARCA 2022)

3. 9.3 [Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery](https://arxiv.org/abs/2308.13135)
* Authors: Patrick Emedom-Nnamdi, Timothy R. Smith, Jukka-Pekka Onnela, Junwei Lu
* Reason: This work proposed a new method for estimating value functions in reinforcement learning, aiming towards making reinforcement learning more interpretable and applicable in medical practice.

4. 8.7 [Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems](https://arxiv.org/abs/2308.13246)
* Authors: Tianchi Cai, Shenliao Bao, Jiyan Jiang, Shiji Zhou, Wenpeng Zhang, Lihong Gu, Jinjie Gu, Guannan Zhang
* Reason: The paper addresses a specific challenge in reinforcement learning-based recommenders, providing two frameworks to improve the performance of these systems.

5. 8.5 [Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity](https://arxiv.org/abs/2308.13278)
* Authors: Achkan Salehi, Stephane Doncieux
* Reason: This paper makes a combination of a Large Language Model (LLM) and a policy that can be shaped by high-level textual prompts, promoting more usable, understandable reinforcement learning algorithm.

