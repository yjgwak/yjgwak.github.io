---
title: Mon, 26 Feb 2024
date: 2024-02-26
---
1. 9.3 [Practice Makes Perfect: Planning to Learn Skill Parameter Policies](https://arxiv.org/abs/2402.15025)
* Authors: Nishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen Proulx, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Jennifer Barry
* Reason: The team of authors includes well-known researchers in the field of AI and robotics, and the paper addresses a significant problem in robot decision-making within complex tasks, which is likely to have a broad impact on both academia and industry.

2. 8.9 [Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control](https://arxiv.org/abs/2402.15194)
* Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, Sergey Levine
* Reason: Involves prominent figure Sergey Levine and presents a novel and potentially influential approach to fine-tuning diffusion models, a topic that has broad applications in many domains.

3. 8.7 [Reinforcement Learning with Elastic Time Steps](https://arxiv.org/abs/2402.14961)
* Authors: Dong Wang, Giovanni Beltrame
* Reason: The paper proposes an innovative approach to time step management in reinforcement learning applied to robotics, which can lead to efficiency enhancements in control algorithms.

4. 8.7 [Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network](https://arxiv.org/abs/2402.15324)
* Authors: Jianhong Wang
* Reason: The paper presents a rigorous theoretical basis for credit assignment in multi-agent reinforcement learning combined with cooperative game theory, which is both foundational and applicable to real-world problems such as energy networks. Its depth (206 pages) suggests a comprehensive contribution to the field.

5. 8.5 [Text Diffusion with Reinforced Conditioning](https://arxiv.org/abs/2402.14843)
* Authors: Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang
* Reason: Despite the broader application in text, it introduces reinforced conditioning which can be a key concept in improving text diffusion models, which are analogous to reinforcement learning strategies.

6. 8.5 [Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation](https://arxiv.org/abs/2402.15399)
* Authors: Zhishuai Liu, Pan Xu
* Reason: This study tackles the significant problem of off-dynamics reinforcement learning and introduces a provably efficient algorithm, which could lead to a deeper understanding of distributionally robust approaches in RL with function approximation.

7. 8.3 [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)
* Authors: Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, et al.
* Reason: The paper proposes Genie, a novel generative model contributing to the development of action-controllable virtual worlds, which is a substantial step towards training generalist agents. The large team of authorities from reputed institutions and the potential for wide applications add to its potential influence.

8. 8.2 [Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization](https://arxiv.org/abs/2402.15197)
* Authors: Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran
* Reason: Presents a model-free Safe RL algorithm, which is a key area of interest in RL, particularly due to its potential application in robotics and safety-critical systems.

9. 8.1 [Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms](https://arxiv.org/abs/2402.15392)
* Authors: Filippo Lazzati, Mirco Mutti, Alberto Maria Metelli
* Reason: It presents novel approaches to inverse reinforcement learning in the offline setting, introducing new concepts and algorithms, which could be foundational for future research in more realistic, offline IRL scenarios.

10. 7.9 [NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks](https://arxiv.org/abs/2402.15393)
* Authors: Bernardo Esteves, Miguel Vasco, Francisco S. Melo
* Reason: The paper extends the capabilities of Deep Thinking methods to both symmetrical and asymmetrical tasks, which is relevant for the scalability and adaptability of machine learning models to complex problems.

