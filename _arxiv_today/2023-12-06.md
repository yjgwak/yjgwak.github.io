---
title: Wed, 6 Dec 2023
date: 2023-12-06
---
1. 9.3 [H-GAP: Humanoid Control with a Generalist Planner](https://arxiv.org/abs/2312.02682)
* Authors: Zhengyao Jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim Rocktäschel, Yuandong Tian
* Reason: High-quality list of authors from prominent institutions, addresses a fundamental problem in reinforcement learning with real-world applications, and demonstrates superiority over baselines and offline RL methods.

2. 9.2 [Training Reinforcement Learning Agents and Humans With Difficulty-Conditioned Generators](https://arxiv.org/abs/2312.02309)
* Authors: Sidney Tio, Jimmy Ho, Pradeep Varakantham
* Reason: Introduces a novel method for both training RL agents and human learners, which can have significant impact on personalized education and adaptive training systems.

3. 8.9 [When is Offline Policy Selection Sample Efficient for Reinforcement Learning?](https://arxiv.org/abs/2312.02355)
* Authors: Vincent Liu, Prabhat Nagarajan, Andrew Patterson, Martha White
* Reason: Discusses the fundamental limits of offline policy selection, a crucial step for deployment of reliable RL systems, and could influence future research on RL sample efficiency and deployment.

4. 8.7 [AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design](https://arxiv.org/abs/2312.02308)
* Authors: Romain Lacombe, Lucas Hendren, Khalid El-Awady
* Reason: Addresses a central problem in clean energy technologies and demonstrates the potential of RL in material science, a field that could substantially benefit from advances in AI.

5. 8.7 [A Q-learning approach to the continuous control problem of robot inverted pendulum balancing](https://arxiv.org/abs/2312.02649)
* Authors: Mohammad Safeea, Pedro Neto
* Reason: Interesting application of Q-learning to a continuous control problem, strong experimental validation, and potential impact on real-world robotic systems.

6. 8.5 [RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum Handoffs, Disconnectivity, and Energy Consumption](https://arxiv.org/abs/2312.02478)
* Authors: Nesrine Cherif, Wael Jaafar, Halim Yanikomeroglu, Abbas Yongacoglu
* Reason: Proposes a RL approach to solve a practical and emerging problem in logistics with UAVs, which has potential applications and impact on the future of autonomous delivery systems.

7. 8.5 [Lights out: training RL agents robust to temporary blindness](https://arxiv.org/abs/2312.02665)
* Authors: N. Ordonez, M. Tromp, P. M. Julbe, W. Böhmer
* Reason: Introduces an innovative concept with significant implications for reinforcement learning in dynamic environments, contributing to the robustness of RL agents.

8. 8.3 [MASP: Scalable GNN-based Planning for Multi-Agent Navigation](https://arxiv.org/abs/2312.02522)
* Authors: Xinyi Yang, Xinting Yang, Chao Yu, Jiayu Chen, Huazhong Yang, Yu Wang
* Reason: Introduces a GNN-based hierarchical planning framework for multi-agent navigation, relevant for complex cooperation strategies and potentially influential in the development of multi-agent systems.

9. 8.2 [Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems](https://arxiv.org/abs/2312.02804)
* Authors: Céline Comte, Matthieu Jonckheere, Jaron Sanders, Albert Senen-Cerda
* Reason: Novel approach to policy-gradient methods which could improve convergence issues in RL, applicable to a variety of important domains.

10. 8.0 [LExCI: A Framework for Reinforcement Learning with Embedded Systems](https://arxiv.org/abs/2312.02739)
* Authors: Kevin Badalian, Lucas Koch, Tobias Brinkmann, Mario Picerno, Marius Wegener, Sung-Yong Lee, Jakob Andert
* Reason: Addresses the significant challenge of RL implementation on embedded systems, which is crucial for practical deployment of RL in real-world scenarios.

