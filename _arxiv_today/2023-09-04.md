---
title: Mon, 4 Sep 2023
date: 2023-09-04
---
1. 9.2 [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267)
* Authors: Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhishek Rastogi
* Reason: The paper presents an innovative approach to reinforcement learning, where AI feedback is used instead of human feedback. The finding that AI feedback can result in similar performance improvements as human feedback is highly promising, especially considering it can overcome the scalability limitations of relying on human preference labels.

2. 8.8 [RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability](https://arxiv.org/abs/2309.00082)
* Authors: Chuning Zhu, Max Simchowitz, Siri Gadipudi, Abhishek Gupta
* Reason: The paper proposes a new approach to visual model-based reinforcement learning which makes the system resilient to spurious variations in the environment. This is a significant development as it is an initiative towards making model-based RL a robust tool for diverse and dynamic domains.

3. 8.5 [End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing](https://arxiv.org/abs/2309.00296)
* Authors: Meraj Mammadov
* Reason: This is a practical application of reinforcement learning to a complex, dynamic problem: autonomous racing. The method used in the paper, with RL driven by lidar and velocity data, and tested in a real-world scenario, represents a significant step forward.

4. 8.3 [How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?](https://arxiv.org/abs/2309.00489)
* Authors: Ahmad M. Nagib, Hatem Abou-Zeid, Hossam S. Hassanein
* Reason: The study investigates not only an important application area for DRL, network resource allocation in O-RAN, but also examines potential performance issues with DRL and suggests remedies. It focuses on practical implementation, enhancing the generalizability of the DRL agents, making it a valuable addition to the recent literature.

5. 8.0 [Multi Agent DeepRL based Joint Power and Subchannel Allocation in IAB networks](https://arxiv.org/abs/2309.00144)
* Authors: Lakshya Jagadish, Banashree Sarma, R. Manivasakan
* Reason: This paper takes an existing methodology, DeepRL, and applies it to the problem of power and subchannel allocation in IAB networks, which is a particularly challenging optimization problem. The multi-agent approach also brings something new to the table and could have a wider impact on other similar applications.

