---
title: Fri, 26 Jan 2024
date: 2024-01-26
---
1. 8.7 [Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation](https://arxiv.org/abs/2401.13884)
* Authors: Yixuan Zhang, Qiaomin Xie
* Reason: Provides both theoretical insights and practical applications for Q-learning with constant stepsizes, showing distributional convergence and enabling the application of extrapolation techniques to improve the estimate of the optimal Q function. High technical depth suggesting significant implications for Q-learning approaches in RL.

2. 8.5 [Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics](https://arxiv.org/abs/2401.13929)
* Authors: Xingche Guo, Donglin Zeng, Yuanjia Wang
* Reason: The paper addresses decision-making dynamics in the context of real-world application to major depressive disorder, introducing an innovative RL-HMM framework. It advances understanding of reward learning in reinforcement learning, which can influence approaches in health-related fields.

3. 8.2 [Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks](https://arxiv.org/abs/2401.14226)
* Authors: Shuai Han, Mehdi Dastani, Shihan Wang
* Reason: Proposes a novel RL algorithm with automatic structuring of reward functions, which is crucial for improving sample efficiency. The use of minimal task-specific knowledge for optimal sub-task selection is likely to influence future research on reward shaping.

4. 8.0 [Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading](https://arxiv.org/abs/2401.13947)
* Authors: Chen Feng, Andrew L. Liu
* Reason: Applies MARL to a highly relevant area of energy trading with the consideration of physical network constraints, merging technical innovation with practical sustainability issues. The real-world implications could be influential for adopting renewable energy trading practices.

5. 7.9 [DittoGym: Learning to Control Soft Shape-Shifting Robots](https://arxiv.org/abs/2401.13231)
* Authors: Suning Huang, Boyuan Chen, Huazhe Xu, Vincent Sitzmann
* Reason: Addresses an emerging area in robotics with the potential to enable advanced morphological control through reinforcement learning. While the concept is novel and futurist, its scope of application and immediate influence might not be as broad as others listed above.

