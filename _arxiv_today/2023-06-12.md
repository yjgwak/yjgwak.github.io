---
title: Mon, 12 Jun 2023
date: 2023-06-12
---
1. 9.6 [On the Importance of Exploration for Generalization in Reinforcement Learning](https://arxiv.org/abs/2306.05483)
    * Authors: Yiding Jiang, J. Zico Kolter, Roberta Raileanu
    * Reason: The authors are highly recognized in the field of Reinforcement Learning(RL), and the paper addresses an essential issue in RL - generalization. They not only theoretically investigate it but also propose a value-based method to tackle the problem.

2. 9.5 [Explaining Reinforcement Learning with Shapley Values](https://arxiv.org/abs/2306.05810)
   * Authors: Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek
   * Reason: Paper provides a theoretical analysis of explaining reinforcement learning using Shapley values. It proposes a framework called SVERL that uses Shapley values to explain agent performance. Accepted in ICML 2023.

3. 9.5 [Neural Algorithmic Reasoning for Combinatorial Optimisation](https://arxiv.org/abs/2306.06064)
   * Authors: Dobrik Georgiev, Danilo Numeroso, Davide Bacciu, Pietro Liò
   * Reason: The authors present a novel approach for addressing combinatorial optimisation problems like Travelling Salesman Problem, a crucial topic in reinforcement learning. The paper achieved superior performance, indicating their approach may have broad applications in solving NP-hard problems using neural networks.

4. 9.3 [On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning](https://arxiv.org/abs/2306.05637)
    * Authors: Hojoon Lee, Koanho Lee, Dongyoon Hwang, Hyunho Lee, Byungkun Lee, Jaegul Choo
    * Reason: This paper addresses the challenge of representational collapse, which is crucial in RL. They propose a novel framework that improves the dimensions of latent representations, with empirical results of significantly improved efficiency, particularly relevant to the Atari 100k benchmark.

5. 9.3 [Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions](https://arxiv.org/abs/2306.05873)
   * Authors: Ezgi Korkmaz, Jonah Brown-Cohen
   * Reason: This work proposes an efficient and novel method for detecting non-robust directions to solve the policy instability problem in reinforcement learning. The research is published in ICML 2023.

6. 9.2 [Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum Markov Games: Switching System Approach](https://arxiv.org/abs/2306.05700)
    * Authors: Donghwan Lee
    * Reason: The author investigates the RL algorithm Minimax Q-learning, offering fresh insights into the field of control theory.  The paper's potential influence is based on the author's quantitative finite-time analysis of both the algorithm itself and the value iteration method.
     
8.9 [In-Sample Policy Iteration for Offline Reinforcement Learning](https://arxiv.org/abs/2306.05726)
    * Authors: Xiaohan Hu, Yi Ma, Chenjun Xiao, Yan Zheng, Zhaopeng Meng
    * Reason: The authors address a crucial problem within RL - offline RL. They propose a novel method which they prove has the ability to learn the in-sample optimal policy, providing a significant advance for offline RL.
     
8.7 [The Role of Diverse Replay for Generalisation in Reinforcement Learning](https://arxiv.org/abs/2306.05727)
    * Authors: Max Weltevrede, Matthijs T.J. Spaan, Wendelin Böhmer
    * Reason: This paper explores the importance of data diversity in replay buffer to improve zero-shot generalization in RL, a critical issue in multi-task RL. The authors provide mathematical motivation and empirical evidence to support their findings.

7. 9.2 [Robust Reinforcement Learning via Adversarial Kernel Approximation](https://arxiv.org/abs/2306.05859)
   * Authors: Kaixin Wang, Uri Gadot, Navdeep Kumar, Kfir Levy, Shie Mannor
   * Reason: The authors propose an approach that approximates the adversarial kernel to enable robust reinforcement learning, making it easily scalable to high-dimensional domains.

8. 9.1 [Virtual Node Tuning for Few-shot Node Classification](https://arxiv.org/abs/2306.06063)
  * Authors: Zhen Tan, Ruocheng Guo, Kaize Ding, Huan Liu
  * Reason: The paper addresses Few-shot Node Classification, a significant problem in graph representation learning. Their approach, Virtual Node Tuning, demonstrates innovation in this area, and has shown superiority in addressing FSNC with unlabeled or sparse labeled base classes.

9. 9.0 [Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning](https://arxiv.org/abs/2306.05769)
   * Authors: Tobias Niehues, Ulla Scheler, Pascal Klink
   * Reason: The paper provides a new regularization method called Self-Paced Absolute Learning Progress (SPALP) to more efficiently use reinforcement learning.

10. 8.9 [Deep Learning for Day Forecasts from Sparse Observations](https://arxiv.org/abs/2306.06079)
  * Authors: Marcin Andrychowicz, Lasse Espeholt, Di Li, Samier Merchant, Alex Merose, Fred Zyda, Shreya Agrawal, Nal Kalchbrenner
  * Reason: Even though it doesn't focus strictly on reinforcement learning, the authors propose MetNet-3, which extends the lead time range and the variables that an observation based neural model can predict well.

11. 8.7 [TreeDQN: Learning to minimize Branch-and-Bound tree](https://arxiv.org/abs/2306.05905)
   * Authors: Dmitry Sorokin, Alexander Kostin
   * Reason: This paper introduces a reinforcement learning method to efficiently learn the branching heuristic on combinatorial optimization problems. The paper is submitted to NeurIPS 2023.

12. 8.7 [CARSO: Counter-Adversarial Recall of Synthetic Observations](https://arxiv.org/abs/2306.06081)
  * Authors: Emanuele Ballarin, Alessio Ansuini, Luca Bortolussi
  * Reason: The authors propose a novel adversarial defence mechanism for image classification. While it's not strictly about reinforcement learning, it offers a defensive architecture that can be adapted to diverse image datasets and classifier architectures.

13. 8.6 [DeepSeaNet: Improving Underwater Object Detection using EfficientDet](https://arxiv.org/abs/2306.06075)
  * Authors: Sanyam Jain
  * Reason: While not strictly based on reinforcement learning, the paper provides a valuable approach for object detection. Using EfficientDet, the proposed model achieved higher accuracy in adversarial noise.

