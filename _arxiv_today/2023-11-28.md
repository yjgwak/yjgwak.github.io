---
title: Tue, 28 Nov 2023
date: 2023-11-28
---
1. 9.1 [A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation](https://arxiv.org/abs/2311.15238)
* Authors: Heyang Zhao, Jiafan He, Quanquan Gu
* Reason: Addresses fundamental RL challenge with minimax optimal regret and near-optimal switching cost. The authors' academic and research standing in machine learning communities indicates high credibility and potential influence on the field.

2. 8.9 [Reinforcement Learning for Wildfire Mitigation in Simulated Disaster Environments](https://arxiv.org/abs/2311.15925)
* Authors: Alexander Tapley, Marissa Dotter, Michael Doyle, Aidan Fennelly, Dhanuj Gandikota, Savanna Smith, Michael Threet, Tim Welsh
* Reason: Accepted for a workshop at NeurIPS 2023, tackles a significant real-world problem (wildfire mitigation), strong interdisciplinary relevance, and provides a publicly available system.

3. 8.7 [Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning](https://arxiv.org/abs/2311.15341)
* Authors: Changyu Chen, Ramesha Karunasena, Thanh Hong Nguyen, Arunesh Sinha, Pradeep Varakantham
* Reason: Accepted in NeurIPS 2023, indicating strong peer recognition, and deals with a complex RL challenge involving large discrete action spaces and validity constraints.

4. 8.7 [Replay across Experiments: A Natural Extension of Off-Policy RL](https://arxiv.org/abs/2311.15951)
* Authors: Dhruva Tirumala, Thomas Lampe, Jose Enrique Chen, Tuomas Haarnoja, Sandy Huang, Guy Lever, Ben Moran, Tim Hertweck, Leonard Hasenclever, Martin Riedmiller, Nicolas Heess, Markus Wulfmeier
* Reason: Contributes to the efficiency and performance of RL algorithms, shown benefits across various algorithms and control domains, and includes authors with known contributions to the RL field.

5. 8.6 [Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines](https://arxiv.org/abs/2311.15960)
* Authors: Yu-An Lin, Chen-Tao Lee, Guan-Ting Liu, Pu-Jen Cheng, Shao-Hua Sun
* Reason: Innovative approach integrating programmatic RL with state machines, showing outperformance on various tasks with the potential for generalization to longer horizons.

6. 8.5 [Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation](https://arxiv.org/abs/2311.16091)
* Authors: Jiachen Li, David Isele, Kanghoon Lee, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer
* Reason: Focus on explainable DRL in a challenging domain of autonomous navigation, integration of novel auxiliary tasks that provides explainable indicators, contribution from authors affiliated with reputable institutions in the field.

7. 8.4 [RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks](https://arxiv.org/abs/2311.15649)
* Authors: Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Dongbin Zhao, He Wang
* Reason: Integrates large language models with robotic planning, showcasing novel insights into generalization and rationality in planning tasks.

8. 8.4 [Should We Learn Most Likely Functions or Parameters?](https://arxiv.org/abs/2311.15990)
* Authors: Shikai Qiu, Tim G. J. Rudner, Sanyam Kapoor, Andrew Gordon Wilson
* Reason: Addresses a fundamental question in machine learning with relevance to RL, offers insights on function-space MAP estimation with potential implications for improving generalization and robustness.

9. 8.0 [Utilizing Explainability Techniques for Reinforcement Learning Model Assurance](https://arxiv.org/abs/2311.15838)
* Authors: Alexander Tapley, Kyle Gatesman, Luis Robaina, Brett Bissey, Joseph Weissman
* Reason: Contributes to the crucial aspect of model assurance via explainability, enhancing trust and adoption in real-world applications.

10. 7.8 [FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots](https://arxiv.org/abs/2311.15327)
* Authors: Akinari Onishi
* Reason: Proposes a novel method to combat user boredom in social robots, an emerging area in RL applications, with practical implications in human-robot interaction.

