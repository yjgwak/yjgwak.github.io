---
title: Tue, 16 Apr 2024
date: 2024-04-16
---
1. 9.2 [The Illusion of State in State-Space Models](https://arxiv.org/abs/2404.08819)
* Authors: William Merrill, Jackson Petty, Ashish Sabharwal
* Reason: The paper challenges the perceived advantage of state-space models in expressive power for state tracking, directly impacting the understanding and design of future large language models. The authors’ previous work on this topic adds to their authority.

2. 9.1 [Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts](https://arxiv.org/abs/2404.09248)
* Authors: Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, Yang Yu
* Reason: Introduces a novel method integrating knowledge from LLMs into RL, enabling agents to perform complex tasks and adapt to novel situations, demonstrating substantial improvements over baselines.

3. 9.0 [WROOM: An Autonomous Driving Approach for Off-Road Navigation](https://arxiv.org/abs/2404.08855)
* Authors: Dvij Kalaria, Shreya Sharma, Sarthak Bhagat, Haoru Xue, John M. Dolan
* Reason: Reinforcement learning (RL) for autonomous driving in off-road environments is a significant and practical application area. The authors' utilization of Proximal Policy Optimization (PPO) alongside Control Barrier Functions (CBF) indicates a sophisticated approach to RL implementation. Given John M. Dolan's authority in the field of autonomous vehicles, this paper has potential for substantial influence.

4. 9.0 [SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents](https://arxiv.org/abs/2404.09331)
* Authors: Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Muhammad Shafique
* Reason: Proposes a framework for designing energy-efficient SNNs for autonomous agents, achieving significant memory savings and speed-up without compromising accuracy, which is crucial for deploying AI in real-world applications.

5. 8.9 [Handling Reward Misspecification in the Presence of Expectation Mismatch](https://arxiv.org/abs/2404.08791)
* Authors: Sarath Sreedharan, Malek Mechergui
* Reason: Tackles a key challenge in AI safety, presenting a novel interactive algorithm with practical solutions to reward misspecification, which is a critical issue in reinforcement learning.

6. 8.9 [Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2404.09715)
* Authors: Linjie Xu, Zichuan Liu, Alexander Dockhorn, Diego Perez-Liebana, Jinyu Wang, Lei Song, Jiang Bian
* Reason: Proposes a practical approach to tackle sample efficiency, a core challenge in MARL, with empirical validations; the authors and their affiliations are recognized in the field which suggests influence.

7. 8.8 [Deep Reinforcement Learning based Online Scheduling Policy for Deep Neural Network Multi-Tenant Multi-Accelerator Systems](https://arxiv.org/abs/2404.08950)
* Authors: Francesco G. Blanco, Enrico Russo, Maurizio Palesi, Davide Patti, Giuseppe Ascia, Vincenzo Catania
* Reason: The paper addresses the contemporary and critical issue of DNNs' cloud-based execution. The relevance of their deep reinforcement learning algorithm, RELMAS, and the significant improvement (173% in SLA satisfaction) shown in the results underscore the paper's importance. The authors' expertise in computer architecture and systems could drive its influence in the sector.

8. 8.8 [Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies](https://arxiv.org/abs/2404.09349)
* Authors: Brian R. Bartoldson, James Diffenderfer, Konstantinos Parasyris, Bhavya Kailkhura
* Reason: Offers a comprehensive analysis of adversarial training scaling laws, predicting a performance plateau and revealing inefficiencies in SOTA methods, thus setting a new direction for future adversarial robustness research.

9. 8.7 [Exploring Text-to-Motion Generation with Human Preference](https://arxiv.org/abs/2404.09445)
* Authors: Jenny Sheng, Matthieu Lin, Andrew Zhao, Kevin Pruvost, Yu-Hui Wen, Yangguang Li, Gao Huang, Yong-Jin Liu
* Reason: Paper accepted to a CVPR workshop, suggesting high relevance, and explores the novel approach of using human preferences in generative motion models.

10. 8.5 [Hindsight PRIORs for Reward Learning from Human Preferences](https://arxiv.org/abs/2404.08828)
* Authors: Mudit Verma, Katherine Metcalf
* Reason: Introduces an innovative credit assignment strategy in PbRL and demonstrates significant performance gains, addressing a core problem in reinforcement learning with practical implications.

11. 8.5 [Active Learning for Control-Oriented Identification of Nonlinear Systems](https://arxiv.org/abs/2404.09030)
* Authors: Bruce D. Lee, Ingvar Ziemann, George J. Pappas, Nikolai Matni
* Reason: Tackles model-based reinforcement learning with a focus on active learning and control, an area with broad applicability. George J. Pappas' standing in control systems and reinforcement learning increases the paper's visibility and potential impact.

12. 8.5 [DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise](https://arxiv.org/abs/2404.09207)
* Authors: Tai Hasegawa, Sukwon Yun, Xin Liu, Yin Jun Phua, Tsuyoshi Murata
* Reason: Introduces a GNN model capable of mitigating noise in both edges and node features, demonstrating robustness against various noise types and the potential for real-world graph data applications.

13. 8.5 [Effective Reinforcement Learning Based on Structural Information Principles](https://arxiv.org/abs/2404.09760)
* Authors: Xianghua Zeng, Hao Peng, Dingli Su, Angsheng Li
* Reason: The paper introduces a novel framework that could substantially improve policy quality, stability, and efficiency in RL; paper could lead to new directions in RL research.

14. 8.4 [Provable Interactive Learning with Hindsight Instruction Feedback](https://arxiv.org/abs/2404.09123)
* Authors: Dipendra Misra, Aldo Pacchiano, Robert E. Schapire
* Reason: Interactive learning, as approached in this paper, is important for practical AI applications. Robert E. Schapire's prominence in machine learning, especially in creating AdaBoost, adds considerable weight to the paper.

15. 8.4 [FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning](https://arxiv.org/abs/2404.09210)
* Authors: Changlin Song, Divya Saxena, Jiannong Cao, Yuqing Zhao
* Reason: Proposes a novel framework for addressing non-iid issues in federated learning, demonstrating significant improvements in accuracy and convergence speed, which could influence future FL systems design.

16. 8.2 [LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning](https://arxiv.org/abs/2404.08767)
* Authors: Junchi Wang, Lei Ke
* Reason: Presents a novel framework that connects segmentation models with large language models, an intersection that can lead to impactful developments in interpretable AI.

17. 8.2 [Mixture of Experts Soften the Curse of Dimensionality in Operator Learning](https://arxiv.org/abs/2404.09101)
* Authors: Anastasis Kratsios, Takashi Furuya, J. Antonio Lara B., Matti Lassas, Maarten de Hoop
* Reason: The paper addresses the curse of dimensionality in operator learning, which is a critical issue in ML. Considering the expertise of the authors in numerical analysis and ML, and the universal approximation premise, this work has a high potential for influence in the ML community.

18. 8.2 [Inferring Behavior-Specific Context Improves Zero-Shot Generalization in Reinforcement Learning](https://arxiv.org/abs/2404.09521)
* Authors: Tidiane Camaret Ndir, André Biedenkapp, Noor Awad
* Reason: Paper addresses zero-shot generalization which is crucial for real-world applications of RL; represents a step towards behavior-specific adaptation.

19. 7.9 [Multiply-Robust Causal Change Attribution](https://arxiv.org/abs/2404.08839)
* Authors: Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman
* Reason: Offers a multiply robust estimation strategy that quantifies the contribution of each causal mechanism to observed changes, relevant to causal inference in reinforcement learning.

20. 7.9 [Hybrid FedGraph: An Efficient Hybrid Federated Learning Algorithm Using Graph Convolutional Neural Network](https://arxiv.org/abs/2404.09443)
* Authors: Jaeyeon Jang, Diego Klabjan, Veena Mendiratta, Fanfei Meng
* Reason: Tackles the less studied yet practically relevant hybrid federated learning scheme, combining it with neural networks for enhanced learning dynamics.

