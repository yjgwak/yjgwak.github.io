---
title: Wed, 20 Mar 2024
date: 2024-03-20
---
1. 8.9 [Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning](https://arxiv.org/abs/2403.12210)
* Authors: Antonio Lopez, David Fridovich-Keil
* Reason: This paper addresses a significant real-world problem of RL's high data requirements in robotics, proposes a novel method for shaping reward functions using DCLFs, and seems to be the first to tackle the computation of DCLFs for high-dimensional systems.

2. 8.9 [Policy Bifurcation in Safe Reinforcement Learning](https://arxiv.org/abs/2403.12847)
* Authors: Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, Keqiang Li
* Reason: Introduces a novel phenomenon in safe RL, policy bifurcation, with thorough theoretical analysis and a new algorithm, which indicates substantial potential influence on the domain of safe RL.

3. 8.7 [Reinforcement Learning from Delayed Observations via World Models](https://arxiv.org/abs/2403.12309)
* Authors: Armin Karamzade, Kyungmin Kim, Montek Kalsi, Roy Fox
* Reason: Offers a novel approach to deal with delayed observations in RL, leveraging world models, which could be very influential given how common delayed feedback is in real-world applications of RL.

4. 8.7 [On Safety in Safe Bayesian Optimization](https://arxiv.org/abs/2403.12948)
* Authors: Christian Fiedler, Johanna Menn, Lukas Kreisk√∂ther, Sebastian Trimpe
* Reason: Addresses critical safety issues in Safe Bayesian Optimization, introduces Real-beta-SafeOpt, and proposes solutions that can strengthen the real-world application of BO under safety constraints.

5. 8.5 [Improving LoRA in Privacy-preserving Federated Learning](https://arxiv.org/abs/2403.12313)
* Authors: Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding
* Reason: Published at a recognized conference (ICLR 2024), addresses the important area of privacy-preserving FL with empirical demonstrations, and improves upon a popular method (LoRA) for federated learning which is of high current interest.

6. 8.5 [Yell At Your Robot: Improving On-the-Fly from Language Corrections](https://arxiv.org/abs/2403.12910)
* Authors: Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn
* Reason: Explores interactive feedback for hierarchical policies in robotic tasks, suggesting the potential for significant impact on the field of robotics and reinforcement learning.

7. 8.3 [Stochastic Halpern iteration in normed spaces and applications to reinforcement learning](https://arxiv.org/abs/2403.12338)
* Authors: Mario Bravo, Juan Pablo Contreras
* Reason: Proposes a method that surpasses the current state-of-the-art in terms of oracle complexity, which is a core challenge in RL, and applies this to MDPs showing potential for practical impact on RL algorithms.

8. 8.3 [D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation](https://arxiv.org/abs/2403.12861)
* Authors: Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner
* Reason: Proposes a new trajectory optimization method for dexterous manipulation, leveraging latent diffusion models, which may influence methods for robotics and RL in complex manipulation tasks.

9. 8.1 [Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion](https://arxiv.org/abs/2403.12406)
* Authors: Kuang-Da Wang, Wei-Yao Wang, Ping-Chun Hsieh, Wen-Chih Peng
* Reason: Focuses on the niche but innovative task of modelling badminton player behavior which could open up new avenues in sports analytics and behavior modelling using hierarchical offline imitation learning.

10. 8.1 [Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes](https://arxiv.org/abs/2403.12946)
* Authors: He Wang, Laixi Shi, Yuejie Chi
* Reason: Discusses sample-efficient distributionally robust policy learning in offline RL, which is crucial for practical applications, indicating considerable relevance in the area of offline RL.

