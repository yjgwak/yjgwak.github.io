---
title: Wed, 8 Nov 2023
date: 2023-11-08
---
1. 9.2 [Toward Reinforcement Learning-based Rectilinear Macro Placement Under Human Constraints](https://arxiv.org/abs/2311.03383)
* Authors: Tuyen P. Le, Hieu T. Nguyen, Seungyeol Baek, Taeyoun Kim, Jungwoo Lee, Seongjung Kim, Hyunjin Kim, Misu Jung, Daehoon Kim, Seokyong Lee, Daewoo Choi
* Reason: This paper addresses a practical and challenging problem in chip design with a novel learning-based framework, and it's associated with an ICCAD 2023 event which indicates a high-profile venue.

2. 9.1 [Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication](https://arxiv.org/abs/2311.03611)
* Authors: Chaofei Fan, Nick Hahn, Foram Kamdar, Donald Avansino, Guy H. Wilson, Leigh Hochberg, Krishna V. Shenoy, Jaimie M. Henderson, Francis R. Willett
* Reason: Significantly advances the field of brain-computer interfaces with long-term stabilization evidence, and involves authors from prestigious institutes, which could imply influential work in this field.

3. 8.9 [PcLast: Discovering Plannable Continuous Latent States](https://arxiv.org/abs/2311.03534)
* Authors: Anurag Koul, Shivakanth Sujit, Shaoru Chen, Ben Evans, Lili Wu, Byron Xu, Rajan Chari, Riashat Islam, Raihan Seraj, Yonathan Efroni, Lekan Molu, Miro Dudik, John Langford, Alex Lamb
* Reason: This paper tackles goal-conditioned planning with a novel representation learning approach, authored by researchers from recognized institutions, and could potentially impact a broad area of RL applications.

4. 8.8 [TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer](https://arxiv.org/abs/2311.03622)
* Authors: Jun Yamada, Marc Rigter, Jack Collins, Ingmar Posner
* Reason: Presents a promising approach for sim-to-real transfer in model-based RL, authored by researchers from a well-regarded robotics group which increases the likely impact of the framework.

5. 8.8 [Proceedings of the 5th International Workshop on Reading Music Systems](https://arxiv.org/abs/2311.04091)
* Authors: Jorge Calvo-Zaragoza, Alexander Pacha, Elona Shatri
* Reason: While not directly aligned with reinforcement learning, the relevance of this paper to a multidisciplinary field and the involvement of senior researchers suggests potential significant influence, especially in how machine learning can be applied to music systems. The unique dataset introduced and the novel approach may translate into advancements in reinforcement learning through cross-disciplinary insights.

6. 8.7 [SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations](https://arxiv.org/abs/2311.03651)
* Authors: Chan Kim, Jaekyung Cho, Christophe Bobda, Seung-Woo Seo, Seong-Woo Kim
* Reason: Addresses an important problem of agents dealing with OOD states and demonstrates substantial improvements over existing methods, which suggests its potential impact in the area of robust RL.

7. 8.6 [Temporal Graph Representation Learning with Adaptive Augmentation Contrastive](https://arxiv.org/abs/2311.03897)
* Authors: Hongjiang Chen, Pengfei Jiao, Huijun Tang, Huaming Wu
* Reason: The paper proposes a novel model (TGAC) that improves temporal graph representation learning, a critical area intersecting with reinforcement learning, especially for environments modeled as graphs with temporal aspects. The proposed adaptive augmentation strategies are innovative and likely to inspire further research.

8. 8.5 [Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)
* Authors: George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi
* Reason: The research targets the safety aspect of machine learning models, which is increasingly becoming critical as such models are deployed in real-world applications. The involvement of multiple authors from industry indicates that the findings could have immediate practical implications, including effects on reinforcement learning models' safety and robustness.

9. 8.3 [Estimator-Coupled Reinforcement Learning for Robust Purely Tactile In-Hand Manipulation](https://arxiv.org/abs/2311.04060)
* Authors: Lennart Röstel, Johannes Pitz, Leon Sievers, Berthold Bäuml
* Reason: This paper is very relevant to reinforcement learning, addressing the challenging task of tactile in-hand manipulation, which is a practical and rapidly advancing domain in robotics. The innovation lies in the coupling of the control policy to the state estimator, which can significantly influence future reinforcement learning algorithms in robotics.

10. 8.1 [Time-Efficient Reinforcement Learning with Stochastic Stateful Policies](https://arxiv.org/abs/2311.04082)
* Authors: Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo
* Reason: The paper directly deals with reinforcement learning and proposes an innovative approach for training stateful policies, addressing issues with the conventional BPTT method. The high relevance of the work to the field, along with the fact that it tackles a fundamental challenge in training stateful RL policies, underlines its potential influence.

