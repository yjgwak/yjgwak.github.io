---
title: Fri, 8 Mar 2024
date: 2024-03-08
---
1. 9.3 [Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems](https://arxiv.org/abs/2403.04007)
* Authors: Wesley A. Suttle, Vipul K. Sharma, Krishna C. Kosaraju, S. Sivaranjani, Ji Liu, Vijay Gupta, Brian M. Sadler
* Reason: This paper presents a novel approach to aligning control theory with reinforcement learning in order to achieve safety guarantees and convergence. Given the affiliation of authors with important institutions and the applicability of the research to safe RL in nonlinear systems, the potential impact in the field of RL is significant.

2. 9.2 [RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning](https://arxiv.org/abs/2403.04344)
* Authors: Boning Li, Zhixuan Fang, Longbo Huang
* Reason: Introduces a novel RL approach for dynamic action abstraction in complex games, demonstrating significant performance improvements over established algorithms, which indicates high potential influence in game theory and complex decision-making applications.

3. 9.0 [A mechanism-informed reinforcement learning framework for shape optimization of airfoils](https://arxiv.org/abs/2403.04329)
* Authors: Jingfeng Wang, Guanghui Hu
* Reason: The paper combines RL with complex engineering processes, specifically for airfoil shape optimization, suggesting impactful applications in design and aerodynamics.

4. 8.9 [Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process](https://arxiv.org/abs/2403.04154)
* Authors: Xiangxin Zhou, Liang Wang, Yichi Zhou
* Reason: The paper addresses a fundamental challenge in policy gradients concerning training stability and sample complexity, and applies it to the significant domain of structure-based drug design, indicating high influence potential in the RL community and beyond.

5. 8.7 [Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer](https://arxiv.org/abs/2403.04001)
* Authors: Suzan Ece Ada, Hanne Say, Emre Ugur, Erhan Oztop
* Reason: The authors propose a unique RL framework that enables effective learning and skill transfer across different robots, showing relevance in multi-task learning and robotics. The affiliation with well-known universities and the innovative nature of the work elevates its importance.

6. 8.7 [Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation](https://arxiv.org/abs/2403.04436)
* Authors: Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi
* Reason: Presents a novel reinforcement learning framework for real-time teleoperation, which is a major step towards practical humanoid robot control and could influence various aspects of robotics and machine learning.

7. 8.5 [SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS](https://arxiv.org/abs/2403.04161)
* Authors: Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun Chang
* Reason: This work presents a novel training-free metric for NAS, which is a crucial element in RL for architecture selection. The impressive correlation with ground-truth performance suggests a potentially transformative impact on the efficiency of NAS and RL.

8. 8.5 [Learning Agility Adaptation for Flight in Clutter](https://arxiv.org/abs/2403.04586)
* Authors: Guangyu Zhao, Tianyue Wu, Yeke Chen, Fei Gao
* Reason: Proposes a learning and planning framework to enhance the flight agility of robots in cluttered environments, offering substantial improvements in efficiency and safety, which have potential implications for autonomous flying robots.

9. 8.3 [Mastering Memory Tasks with World Models](https://arxiv.org/abs/2403.04253)
* Authors: Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar
* Reason: By tackling the issue of long-term dependencies and long-horizon credit assignment in RL, this paper could significantly affect how complex memory tasks are approached in the field. The fact that it establishes superhuman benchmarks further underscores its potential influence.

10. 8.3 [Teaching Large Language Models to Reason with Reinforcement Learning](https://arxiv.org/abs/2403.04642)
* Authors: Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu
* Reason: Investigates the improvement of reasoning capabilities in large language models through various reinforcement learning algorithms, which might influence future research in natural language processing and AI alignment.

