---
title: Thu, 6 Jul 2023
date: 2023-07-06
---
1. 9.8 [Causal Reinforcement Learning: A Survey](https://arxiv.org/abs/2307.01452)
* Authors: Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang
* Reason: The authors have a solid background and reputation in the field. The paper tackles the emerging yet crucial intersection of reinforcement learning with causality, providing a comprehensive review of the literature and paving the way for future research directions.

2. 9.3 [Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach](https://arxiv.org/abs/2307.01316)
* Authors: Iman Sharifi, Mustafa Yildirim, Saber Fallah
* Reason: The paper tackles a critical challenge in the application of deep reinforcement learning in real world scenarios focusing on autonomous driving. The use of first-order symbolic logics for safety in the proposed model is key here.

3. 9.2 [Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing](https://arxiv.org/abs/2307.01995)
* Authors: Qiulei Wang, Lei Yan, Gang Hu, Wenli Chen, Jean Rabault, Bernd R. Noack
* Reason: Significant improvement in DRL performance, applicability to real-world control tasks, and solid grounding in reinforcement learning methodology.

4. 9.0 [Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning](https://arxiv.org/abs/2307.01472)
* Authors: Zhuoran Li, Ling Pan, Longbo Huang
* Reason: The paper presents a novel approach to offline Multi-Agent Reinforcement Learning, providing significant improvements in performance, generalization and data-efficiency over existing methods.

5. 8.9 [Deep Attention Q-Network for Personalized Treatment Recommendation](https://arxiv.org/abs/2307.01519)
* Authors: Simin Ma, Junghwan Lee, Nicoleta Serban, Shihao Yang
* Reason: The paper focuses on the important area of personalized medicine, proposing a transformative neural network model that can lead to better healthcare outcomes.

6. 8.9 [Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization](https://arxiv.org/abs/2307.02108)
* Authors: Sanath Kumar Krishnamurthy, Ruohan Zhan, Susan Athey, Emma Brunskill
* Reason: Innovative approach to simple regret minimization, flexible application to a range of stochastic contextual bandit settings, and insightful negative result on regret guarantees.

7. 8.7 [Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics](https://arxiv.org/abs/2307.01770)
* Authors: Guillaume Mahey, Laetitia Chapel, Gilles Gasso, Cl√©ment Bonet, Nicolas Courty
* Reason: This paper presents a new algorithm for faster calculation of the Wasserstein distance, which is crucial for reinforcement learning algorithms. Incremental improvements like these often have great impact.

8. 8.7 [DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks](https://arxiv.org/abs/2307.02159)
* Authors: Jingwei Zhang, Han Shi, Jincheng Yu, Enze Xie, Zhenguo Li
* Reason: Proposes a unified theoretic framework for SDMs and GANs, with potential for new algorithms achieving a better balance between high sample quality and sampling speed.

9. 8.5 [Personalized Federated Learning via Amortized Bayesian Meta-Learning](https://arxiv.org/abs/2307.02222)
* Authors: Shiyu Liu, Shaogao Lv, Dun Zeng, Zenglin Xu, Hui Wang, Yue Yu
* Reason: Offers a novel approach to personalized federated learning and demonstrates superior performance over competitive baselines in experimental results.

10. 8.3 [Meta-Learning Adversarial Bandit Algorithms](https://arxiv.org/abs/2307.02295)
* Authors: Mikhail Khodak, Ilya Osadchiy, Keegan Harris, Maria-Florina Balcan, Kfir Y. Levy, Ron Meir, Zhiwei Steven Wu
* Reason: First to address online meta-learning with bandit feedback, providing theoretical insights and practical algorithms for MAB and BLO.

