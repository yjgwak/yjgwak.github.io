---
title: Mon, 25 Mar 2024
date: 2024-03-25
---
1. 9.3 [Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies](https://arxiv.org/abs/2403.15267)
* Authors: Nicolò Botteghi, Urban Fasel
* Reason: Introduces a novel approach to solving complex parametric PDE control problems using DRL, which has wide applications in engineering and science. The method's performance and potential interpretability of the learned policies add to its influence.

2. 9.2 [Planning with a Learned Policy Basis to Optimally Solve Complex Tasks](https://arxiv.org/abs/2403.15301)
* Authors: Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof
* Reason: Proposes a novel combination of successor features and (sub)policies in reinforcement learning to optimally solve complex tasks with non-Markovian reward specifications, imperative for AI's generalization capability.

3. 9.1 [Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control](https://arxiv.org/abs/2403.14860)
* Authors: Minjun Sung, Sambhu H. Karumanchi, Aditya Gahlawat, Naira Hovakimyan
* Reason: Introduces control-theoretic augmentation for MBRL, improving robustness against uncertainties, which is critical for real-world application of reinforcement learning

4. 8.9 [DP-Dueling: Learning from Preference Feedback without Compromising User Privacy](https://arxiv.org/abs/2403.15045)
* Authors: Aadirupa Saha, Hilal Asi
* Reason: Introduces a novel differentially private algorithm for the dueling bandit problem, with near-optimal performance which is highly relevant in privacy-centric reinforcement learning applications.

5. 8.9 [Spectral Motion Alignment for Video Motion Transfer using Diffusion Models](https://arxiv.org/abs/2403.15249)
* Authors: Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye
* Reason: This work is on the cutting edge of video generation with implications for the fields of computer vision and AI, especially given the increasing interest in deepfakes and motion capture technologies.

6. 8.8 [DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data](https://arxiv.org/abs/2403.15389)
* Authors: Hanrong Ye, Dan Xu
* Reason: Addresses the significant problem of learning from partially annotated data for multiple tasks and proposes a novel diffusion framework, crucial for real-world applications where complete annotation is costly or infeasible.

7. 8.7 [Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning](https://arxiv.org/abs/2403.14690)
* Authors: Xiuqin Zhong, Shengyuan Yan, Gongqi Lin, Hongguang Fu, Liang Xu, Siwen Jiang, Lei Huang, Wei Fang
* Reason: Presents a novel DRL framework with attention mechanisms showing substantial improvement in precision, with potential impact on both AI research and online education

8. 8.6 [Automated Feature Selection for Inverse Reinforcement Learning](https://arxiv.org/abs/2403.15079)
* Authors: Daulet Baimukashev, Gokhan Alcan, Ville Kyrki
* Reason: Addresses an important problem in reinforcement learning (reward specification) with a practical feature selection method that could enhance the effectiveness of IRL applications.

9. 8.6 [Controlled Training Data Generation with Diffusion Models](https://arxiv.org/abs/2403.15309)
* Authors: Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir
* Reason: Explores an innovative closed-loop system for generating controlled training data using generative models, with potential to address the challenge of domain adaptation in supervised learning models.

10. 8.4 [Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection](https://arxiv.org/abs/2403.14797)
* Authors: Gaurav Bhatt, James Ross, Leonid Sigal
* Reason: Addresses catastrophic forgetting in continual learning for complex vision tasks, which is a major challenge in the field, with significant improvements demonstrated

11. 8.3 [Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning](https://arxiv.org/abs/2403.15091)
* Authors: Esmaeel Mohammadi, Daniel Ortiz-Arroyo, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Petar Durdevic
* Reason: Proposes significant improvements in LSTM-based simulators which can be crucial for applying DRL in complex industrial processes, indicating potential for impactful practical applications.

12. 8.2 [Continual Learning by Three-Phase Consolidation](https://arxiv.org/abs/2403.14679)
* Authors: Davide Maltoni, Lorenzo Pellegrini
* Reason: Introduces a novel approach to continual learning that controls forgetting, possessing accuracy and efficiency advantages over existing approaches, relevant for lifelong learning systems

13. 8.1 [Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement](https://arxiv.org/abs/2403.15180)
* Authors: Jonathan Pirnay, Dominik G. Grimm
* Reason: Presents a new method that merges supervised and unsupervised learning to improve solutions in combinatorial optimization, with potential influence in the optimization domain of reinforcement learning.

14. 7.9 [Foundation Models for Time Series Analysis: A Tutorial and Survey](https://arxiv.org/abs/2403.14735)
* Authors: Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen
* Reason: Provides an extensive overview of Foundation Models for time series, which are gaining prominence in the field, with clear potential to influence future research directions in time series analysis

15. 7.8 [PDE-CNNs: Axiomatic Derivations and Applications](https://arxiv.org/abs/2403.15182)
* Authors: Gijs Bellaard, Sei Sakata, Bart M. N. Smets, Remco Duits
* Reason: While not strictly about reinforcement learning, this paper proposes a PDE-based approach to convolutional neural networks that could influence the development of architectures in RL tasks that require image processing.

