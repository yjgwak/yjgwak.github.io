---
title: Thu, 27 Jul 2023
date: 2023-07-27
---
1. 9.3 [Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks](https://arxiv.org/abs/2307.14085)
* Authors: Siyu Chen, Mengdi Wang, Zhuoran Yang
* Reason: This paper discusses reinforcement learning (RL) from a strategic perspective, detailing a unique approach to encourage efficient decision-making within Markov games. The authors also propose sample-efficient algorithms for online and offline functions, and their novel approach can have significant potential to influence the field of RL.

2. 8.9 [Offline Reinforcement Learning with On-Policy Q-Function Regularization](https://arxiv.org/abs/2307.13824)
* Authors: Laixi Shi, Robert Dadashi, Yuejie Chi, Pablo Samuel Castro, Matthieu Geist
* Reason: This paper solves a fundamental problem in offline reinforcement learning (RL), which has been published at a reputable conference, European Conference on Machine Learning (ECML). It introduces a new pathway to manage the distribution shift between historical data and the desired policy.

3. 8.5 [Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation](https://arxiv.org/abs/2307.13978)
* Authors: Mahyar Abbasian, Taha Rajabzadeh, Ahmadreza Moradipari, Seyed Amir Hossein Aqajari, Hongsheng Lu, Amir Rahmani
* Reason: This paper merges the fields of GAN and RL, introducing a reinforcement learning (RL) agent that can navigate within the latent space of a GAN to produce desired outputs. It's an innovative methodology with potential significant influence on generative networks.

4. 8.2 [FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning](https://arxiv.org/abs/2307.13716)
* Authors: Leiming Chen, Cihao Dong, Sibo Qiao, Ziling Huang, Kai Wang, Yuming Nie, Zhaoxiang Hou, Cheewei Tan
* Reason: This paper provides an innovative fusion approach to handle the heterogeneity issues in the models of each client in federated learning. It makes good use of reinforcement learning, and the experimental results show promising results.

5. 7.9 [A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch](https://arxiv.org/abs/2307.14304)
* Authors: Shengren Hou, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara
* Reason: The authors propose a deep reinforcement learning framework that can handle operational constraints in energy storage systems, bridging the gap between RL and energy system operations. It is a very specific application of reinforcement learning with practical impacts.

6. 7.7 [Reinforcement Learning by Guided Safe Exploration](https://arxiv.org/abs/2307.14316)
* Authors: Qisong Yang, Thiago D. Sim√£o, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan
* Reason: This paper provides a novel approach to training RL agents safely in an unknown environment. Despite its contribution, it is placed lower due to the more niche area of safety within reinforcement learning.

