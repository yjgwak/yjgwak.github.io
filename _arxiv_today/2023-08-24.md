---
title: Thu, 24 Aug 2023
date: 2023-08-24
---
1. 9.8 [Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network](https://arxiv.org/abs/2308.11771)
* Authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao
* Reason: This paper offers advancements in eye-tracking technology that could be transformative for AR/VR headsets in healthcare. The method uses low-latency, sparse output event cameras and introduces a novel architecture, CB-ConvLSTM, to enhance efficiency in extracting spatio-temporal features for pupil tracking. It seems ideal for real-time eye-tracking in resource-constrained devices.

2. 9.6 [${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2308.11842)
* Authors: Dingyang Chen, Qi Zhang
* Reason: The authors find a unique way to exploit Euclidean symmetries in cooperative multi-agent reinforcement learning tasks. The work introduces symmetric constraints into neural network architectures to enhance the performance in different benchmarks and has potential for zero-shot learning and transfer learning.

3. 9.5 [Diverse Policies Converge in Reward-free Markov Decision Processes](https://arxiv.org/abs/2308.11924)
* Authors: Fanqi Lin, Shiyu Huang, Weiwei Tu
* Reason: The paper provides a unified diversity reinforcement learning framework and investigates the convergence of training diverse policies. This topic is very relevant to the current state of the field where diversity in policy learning is an emerging research area.

4. 9.4 [Accelerating Exact Combinatorial Optimization via RL-based Initialization -- A Case Study in Scheduling](https://arxiv.org/abs/2308.11652)
* Authors: Jiaqi Yin, Cunxi Yu
* Reason: This paper appears to be highly innovative and could have significant impact in machine learning applications for combinatorial optimization. They provide a two-phase reinforcement learning-to-integer linear programming (RL-to-ILP) framework for scheduling tasks and demonstrate significant speed improvements.

5. 9.1 [Knowledge Graph Prompting for Multi-Document Question Answering](https://arxiv.org/abs/2308.11730)
* Authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr
* Reason: This paper proposes a novel knowledge graph prompting method for multi-document question answering. Combining this graph construction to formulate the context in prompting large language models shows the potential of leveraging graphs in enhancing the prompt design for such models.

6. 9.0 [Maintaining Plasticity via Regenerative Regularization](https://arxiv.org/abs/2308.11958)
* Authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy
* Reason: The authors propose a novel and simple method for maintaining plasticity in continual learning, an important area in machine learning. Their proposal has the potential to be largely influential over continual learning research.

7. 8.7 [A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data](https://arxiv.org/abs/2308.11849)
* Authors: Enze Liu, Zhiyuan Lin, Judith Y.T. Wang, Hong Chen
* Reason: This paper addresses a timely and complex problem of railway rescheduling, especially during disruptions, with reinforcement learning. The approach uses mobile data to capture real-time passenger mobility, which demonstrates an innovative use of data-driven methods in mitigating station overcrowding.

8. 8.6 [Learning to Learn Financial Networks for Optimising Momentum Strategies](https://arxiv.org/abs/2308.12212)
* Authors: Xingyue, Stefan Zohren, Stephen Roberts, Xiaowen Dong
* Reason: Authors propose an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The methods employed have great potential for influencing both the field of machine learning and finance.

9. 8.2 [Language Reward Modulation for Pretraining Reinforcement Learning](https://arxiv.org/abs/2308.12270)
* Authors: Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, Pieter Abbeel
* Reason: The authors propose a novel way to leverage the capabilities of Learned Reward Functions (LRFs) as a pretraining signal for Reinforcement Learning (RL). Their findings and suggestions could bring changes to current pretraining practices in RL.

10. 7.8 [Prompt-Based Length Controlled Generation with Reinforcement Learning](https://arxiv.org/abs/2308.12030)
* Authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu
* Reason: This paper demonstrates the effectiveness of applying reinforcement learning to the emerging field of length-controlled generative language models. Although it's more niche in focus, it's still a robust application of reinforcement learning in natural language processing.

