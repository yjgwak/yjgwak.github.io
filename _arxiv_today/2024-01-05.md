---
title: Fri, 5 Jan 2024
date: 2024-01-05
---
1. 8.9 [Trajectory-Oriented Policy Optimization with Sparse Rewards](https://arxiv.org/abs/2401.02225)
* Authors: Guojian Wang, Faguo Wu, Xiao Zhang
* Reason: Addresses the significant challenge of sparse rewards in DRL with a novel approach, and the empirical evidence showing it outperforms baseline methods extensively in both discrete and continuous tasks, supporting its potential high impact.

2. 8.7 [Policy-regularized Offline Multi-objective Reinforcement Learning](https://arxiv.org/abs/2401.02244)
* Authors: Qian Lin, Chao Yu, Zongkai Liu, Zifan Wu
* Reason: Tackles the less explored domain of multi-objective RL using offline data and introduces innovative solutions to preference-inconsistent demonstrations, indicated to be effective by empirical results thus could influence future studies in offline MORL.

3. 8.5 [A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning](https://arxiv.org/abs/2401.02325)
* Authors: Parvin Malekzadeh, Konstantinos N. Plataniotis, Zissis Poulos, Zeyu Wang
* Reason: Offers a generalization of a widely-used loss function in distributional RL with the potential to enhance robustness and adjustability, demonstrated on standard benchmarks; this advancement could be significant for RL applications with noisy or outlier data.

4. 8.3 [A Survey Analyzing Generalization in Deep Reinforcement Learning](https://arxiv.org/abs/2401.02349)
* Authors: Ezgi Korkmaz
* Reason: Provides a comprehensive review of the generalization problems in deep RL and suggests unifying various solution approaches, which may influence future research directions in creating more robust policies.

5. 8.1 [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation](https://arxiv.org/abs/2401.02117)
* Authors: Zipeng Fu, Tony Z. Zhao, Chelsea Finn
* Reason: Presents an innovative system for teaching mobile manipulation to robots with successful demonstrations of complex tasks, the involvement of Chelsea Finn, a recognized authority in the field of robot learning, contributes to the paper's influence.

