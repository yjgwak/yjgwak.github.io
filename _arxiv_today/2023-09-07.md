---
title: Thu, 7 Sep 2023
date: 2023-09-07
---
1. 9.2 [Deep Reinforcement Learning from Hierarchical Weak Preference Feedback](https://arxiv.org/abs/2309.02632)
* Authors: Alexander Bukharin, Yixiao Li, Pengcheng He, Weizhu Chen, Tuo Zhao
* This paper introduces a new framework, HERON, for learning reward functions for complicated tasks, which may carry additional benefits like improved sample efficiency and resilience. This novelty in the field of Deep Reinforcement Learning could revolutionize the efficiency and effectiveness of the learning process, providing a significant influence on future research.

2. 9.1 [Active flow control for three-dimensional cylinders through deep reinforcement learning](https://arxiv.org/abs/2309.02462)
* Authors: Pol Suárez, Francisco Alcántara-Ávila, Arnau Miró, Jean Rabault, Bernat Font, Oriol Lehmkuhl, R. Vinuesa
* The paper addresses the challenge of active flow control for three-dimensional cylinders by employing a deep-reinforcement-learning framework. Its unique approach makes it adaptable to different geometries and accelerates training, which gives it the potential to have a significant influence in the field of Active Flow Control.

3. 9.1 [Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models](https://arxiv.org/abs/2309.02976)
* Authors: Pierre Schumacher, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle
* Reason: This paper presents a novel framework for robust bipedal walking, a fundamental research problem in reinforcement learning domain. The authors are experts in the field of reinforcement learning and robotics, and the proposed method outperforms others that rely heavily on expert data sets.

4. 9.0 [Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning](https://arxiv.org/abs/2309.02669)
* Authors: Tianchi Cai, Jiyan Jiang, Wenpeng Zhang, Shiji Zhou, Xierui Song, Li Yu, Lihong Gu, Xiaodong Zeng, Jinjie Gu, Guannan Zhang
* The work introduces a novel approach to marketing budget allocation utilizing Offline Constrained Deep Reinforcement Learning. As this paper also establishes a method to address the challenge faced by RLHF due to the cost of human preference data, it has the potential to influence future research and development in marketing budget allocation.

5. 8.9 [A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges](https://arxiv.org/abs/2309.02473)
* Authors: Maryam Zare, Parham M. Kebria, Abbas Khosravi, Saeid Nahavandi
* This survey of imitation learning delivers an overview of its assumptions, approaches, and its recent advances. It presents a comprehensive guide to the field and discusses current challenges and future research paths which makes it a useful source for researchers in this area.

6. 8.9 [ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning](https://arxiv.org/abs/2309.03081)
* Authors: Linkang Du, Min Chen, Mingyang Sun, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang
* Reason: This paper tackles an important and challenging issue in offline DRL: auditing datasets. The authors introduce a unique dataset auditing mechanism for offline RL scenarios. Their solution has been demonstrated to be highly effective.

7. 8.8 [Subgraph Attention Networks for Molecular Graph Property Prediction and Feature Interpretation](https://arxiv.org/abs/2204.09346)
* Authors: Yinhao Zhu, Jiajie Qu, Dianqi Li, Tarun Kumar, Guoyong Yu, Lu Li, Rishi Gupta, Bhuwan Dhingra, Soumik Sarkar, Yuxiao Dong, Linxiao Li
* The paper proposes Subgraph Attention Networks for molecular graph property prediction, which provides an interpretable model for molecular representations. This works as an effective feature interpretation technique in chemistry and could influence future research in the related area.

8. 8.5 [Pure Monte Carlo Counterfactual Regret Minimization](https://arxiv.org/abs/2309.03084)
* Authors: Ju Qi, Ting Feng, Falun Hei, Zhemei Fang, Yunfeng Luo
* Reason: This paper proposes a new algorithm for resolving large-scale incomplete information games, a highly researched topic in reinforcement learning. The authors' technique advanced the solutions in the domain and achieved faster convergence.

9. 8.2 [On Reducing Undesirable Behavior in Deep Reinforcement Learning Models](https://arxiv.org/abs/2309.02869)
* Authors: Ophir Carmel, Guy Katz
* Reason: This paper contributes to the field of reinforcement learning by introducing a novel framework to reduce undesirable behavior in DRL-based software. The method improved performance while significantly reducing undesirable behavior.

10. 8.0 [Rethinking Momentum Knowledge Distillation in Online Continual Learning](https://arxiv.org/abs/2309.02870)
* Authors: Nicolas Michel, Maorong Wang, Ling Xiao, Toshihiko Yamasaki
* Reason: This paper made a significant contribution to Online Continual Learning, improving existing state-of-the-arts accuracy by over 10%. The authors have thoroughly examined the role of Momentum Knowledge Distillation in this domain and proposed an effective methodology for its application.

