---
title: Wed, 29 Nov 2023
date: 2023-11-29
---
1. 8.9 [Multi-Agent Learning of Efficient Fulfilment and Routing Strategies in E-Commerce](https://arxiv.org/abs/2311.16171)
* Authors: Omkar Shelke, Pranavi Pathakota, Anandsingh Chauhan, Harshad Khadilkar, Hardik Meisheri, Balaraman Ravindran
* Reason: The paper integrates reinforcement learning with graph neural networks and tackles a complex, real-world problem, which is commercially significant. The authors are from reputable institutions and have expertise in AI, likely making this work influential in both academic and industrial applications.

2. 8.7 [Evolutionary Machine Learning and Games](https://arxiv.org/abs/2311.16172)
* Authors: Julian Togelius, Ahmed Khalifa, Sam Earle, Michael Cerny Green, Lisa Soros
* Reason: Evolutionary algorithms are a key area in reinforcement learning, and this paper's focus on games suggests methods that are often at the forefront of RL research. The authors include leading researchers in the field, making it potentially influential for new techniques in EML.

3. 8.6 [Value Approximation for Two-Player General-Sum Differential Games with State Constraints](https://arxiv.org/abs/2311.16520)
* Authors: Lei Zhang, Mukesh Ghimire, Wenlong Zhang, Zhe Xu, Yi Ren
* Reason: Addresses the curse of dimensionality in differential games which is a central challenge in reinforcement learning, and provides solutions with practical implications in robotics.

4. 8.5 [Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs](https://arxiv.org/abs/2311.16151)
* Authors: Thomas Summe, Clemens JS Schaefer, Siddharth Joshi
* Reason: The paper presents an online training method for SNNs which are gaining interest in RL for their biological plausibility and efficiency. It confronts a core issue (scaling to deep models) in SNN research, increasing its relevance and potential impact on the field.

5. 8.4 [Digital Twin-Enhanced Deep Reinforcement Learning for Resource Management in Networks Slicing](https://arxiv.org/abs/2311.16876)
* Authors: Zhengming Zhang, Yongming Huang, Cheng Zhang, Qingbi Zheng, Luxi Yang, Xiaohu You
* Reason: Proposes an innovative digital twin framework for deep reinforcement learning in network resource management, showing potential for significant improvements in performance.

6. 8.3 [A Graph Neural Network-Based QUBO-Formulated Hamiltonian-Inspired Loss Function for Combinatorial Optimization using Reinforcement Learning](https://arxiv.org/abs/2311.16277)
* Authors: Redwan Ahmed Rizvee, Raheeb Hasan, Md. Mosaddek Khan
* Reason: The paper's method of applying QUBO with reinforcement learning to combinatorial optimization problems is original and suggests cross-application potential between classical optimization problems and RL methods. The work could be influential for those bridging these research areas.

7. 8.2 [Goal-conditioned Offline Planning from Curious Exploration](https://arxiv.org/abs/2311.16996)
* Authors: Marco Bagatella, Georg Martius
* Reason: Introduces a method for extracting goal-conditioned behavior from unsupervised exploration, enhancing offline reinforcement learning approaches.

8. 8.1 [Reward Shaping for Improved Learning in Real-time Strategy Game Play](https://arxiv.org/abs/2311.16339)
* Authors: John Kliem, Prithviraj Dasgupta
* Reason: Reward shaping is a critical component of RL, and its application to complex real-time games could have broad implications for training more effective RL agents. The paper demonstrates practical improvements in the learning process, which could be influential to RL practices in gaming.

9. 7.9 [Adversarial Distribution Balancing for Counterfactual Reasoning](https://arxiv.org/abs/2311.16616)
* Authors: Stefan Schrod, Fabian Sinz, Michael Altenbuchinger
* Reason: Tackles the challenge in causal prediction models, which is closely related to reinforcement learning in terms of dealing with unobserved outcomes and distributional differences.

10. 7.7 [An Investigation of Time Reversal Symmetry in Reinforcement Learning](https://arxiv.org/abs/2311.17008)
* Authors: Brett Barkley, Amy Zhang, David Fridovich-Keil
* Reason: Investigates the concept of time reversal in the context of Markov decision processes, providing a potentially new avenue for enhancing the efficiency of reinforcement learning algorithms.

