---
title: Thu, 22 Jun 2023
date: 2023-06-22
---
1. 9.6 [Efficient Dynamics Modeling in Interactive Environments with Koopman Theory](https://arxiv.org/abs/2306.11941)
* Authors: Arnab Kumar Mondal, Siba Smarak Panigrahi, Sai Rajeswar, Kaleem Siddiqi, Siamak Ravanbakhsh
* Reason: The authors present a novel approach to model dynamics in interactive environments, an important aspect of reinforcement learning. Their approach promises significant improvements in efficiency and accuracy over existing models, indicating high potential influence on future work in the field. Furthermore, the authors have a strong academic background, increasing the likelihood of the paper's influence.

2. 9.6 [Optimistic Active Exploration of Dynamical Systems](https://arxiv.org/abs/2306.12371)
* Authors: Bhavya Sukhija, Lenart Treven, Cansu Sancaktar, Sebastian Blaes, Stelian Coros, Andreas Krause
* Reason: Due to the practical application of reinforcement learning in real-life dynamical systems. Furthermore, the authors have good reputation in the field of reinforcement learning, increasing the potential influence of the paper.

3. 9.5 [Learning to Generate Better Than Your LLM](https://arxiv.org/abs/2306.11816)
* Authors: Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun
* Reason: This paper introduces a new suite of reinforcement learning algorithms for fine-tuning large language models, showing considerable improvement in performance over existing algorithms. It is highly influential due to its detailed investigation of the interaction between RL algorithms and a dynamic guide LLM.

4. 9.4 [Reward Shaping via Diffusion Process in Reinforcement Learning](https://arxiv.org/abs/2306.11885)
* Authors: Peeyush Kumar
* Reason: This study presents an innovative framework for reward shaping in RL, using principles from stochastic thermodynamics and system dynamics. It provides a novel perspective on the physical nature of information and its implications for online learning in MDPs, which could have a large impact on the understanding of information-oriented formulations in RL.

5. 9.4 [AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization](https://arxiv.org/abs/2306.11971)
* Authors: Maziar Gomrokchi, Owen Levin, Jeffrey Roach, Jonah White
* Reason: The authors introduced a novel benchmark environment for reinforcement learning, which could greatly influence future studies in this domain. They demonstrate how real-world uncertainties can be effectively managed in a complex, dynamic environment, showing promise for practical applications in reinforcement learning.

6. 9.2 [SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling](https://arxiv.org/abs/2306.11886)
* Authors: Jesse Zhang, Karl Pertsch, Jiahui Zhang, Joseph J. Lim
* Reason: This paper proposes a scalable approach to pre-training a diverse set of skills for robots, which significantly reduces the human effort needed. The proposed method SPRINT paves the way for accelerating learning new long-horizon tasks in a real-world setting, offering vast potential impact in robotics applications.

7. 9.2 [One-shot Imitation Learning via Interaction Warping](https://arxiv.org/abs/2306.12392)
* Authors: Ondrej Biza, Skye Thompson, Kishore Reddy Pagidi, Abhinav Kumar, Elise van der Pol, Robin Walters, Thomas Kipf, Jan-Willem van de Meent, Lawson L.S. Wong, Robert Platt
* Reason: The novelty of learning SE(3) robotic manipulation policies from a single demonstration may lead it to be very influential in the area of robotic learning.

8. 9.1 [Randomized Quantization is All You Need for Differential Privacy in Federated Learning](https://arxiv.org/abs/2306.11913)
* Authors: Yeojoon Youn, Zihao Hu, Juba Ziani, Jacob Abernethy
* Reason: This paper presents an innovative federated learning method, RQM, that uniquely combines quantization and differential privacy for a robust solution to concerns about privacy and computational complexity in decentralized settings. The method shows significantly improved privacy-accuracy trade-offs, likely resulting in a high impact on the development of practical Federated Learning systems.

9. 9.1 [Introspective Action Advising for Interpretable Transfer Learning](https://arxiv.org/abs/2306.12314)
* Authors: Joseph Campbell, Yue Guo, Fiona Xie, Simon Stepputtis, Katia Sycara
* Reason: The authors propose an approach to transfer learning in deep reinforcement learning that promises to improve convergence rates and is transferable to alternate models. Their work could influence the next wave of research on transfer learning in reinforcement learning.

10. 9.0 [Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback](https://arxiv.org/abs/2306.11918)
* Authors: Hang Wang, Sen Lin, Junshan Zhang
* Reason: The paper presents a groundbreaking ensemble method for Q-learning to mitigate overestimation issues. It proposes an adaptive approach based on the feedback of approximation errors for dynamically controlling the ensemble size. The method, AdaEQ, has shown promising results in the MuJoCo benchmark, suggesting potential influence on enhancing the performance of Q-learning algorithms.

11. 8.8 [Provably Efficient Representation Learning with Tractable Planning in Low-Rank POMDP](https://arxiv.org/abs/2306.12356)
* Authors: Jiacheng Guo, Zihao Li, Huazheng Wang, Mengdi Wang, Zhuoran Yang, Xuezhou Zhang
* Reason: Although a bit specialized within reinforcement learning, the authors present efficient algorithms for representation learning in POMDPs. The provable efficiency combined with computational tractability contributes to its potential influence in this research domain.

12. 8.7 [Timely Asynchronous Hierarchical Federated Learning: Age of Convergence](https://arxiv.org/abs/2306.12400)
* Authors: Purbesh Mitra, Sennur Ulukus
* Reason: Applications of reinforcement learning to federated learning and its demonstration of probabilistic advantages in client optimization. Author affiliations also indicate potential influence in the field.

13. 8.5 [Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training](https://arxiv.org/abs/2306.12230)
* Authors: Aleksandra I. Nowak, Bram Grooten, Decebal Constantin Mocanu, Jacek Tabor
* Reason: This paper focuses on dynamic sparse training, an important aspect of reinforcement learning. While a bit specialized, it provides comprehensive experimental analysis and findings that could potentially influence future studies on sparse training and pruning techniques.

14. 8.3 [Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms](https://arxiv.org/abs/2306.12380)
* Authors: Qian Yu, Yining Wang, Baihe Huang, Qi Lei, Jason D. Lee
* Reason: The focus on stochastic zeroth-order optimization has great potential impacts for complex machine learning models.

15. 8.1 [Addressing Discontinuous Root-Finding for Subsequent Differentiability in Machine Learning, Inverse Problems, and Control](https://arxiv.org/abs/2306.12413)
* Authors: Daniel Johnson, Ronald Fedkiw
* Reason: The paper proposes a novel numerical approach to solve discontinuities in physical processes - a problem that presents significant challenges in reinforcement learning applications.

