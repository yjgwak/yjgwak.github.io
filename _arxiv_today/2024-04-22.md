---
title: Mon, 22 Apr 2024
date: 2024-04-22
---
1. 8.7 [MAexp: A Generic Platform for RL-based Multi-Agent Exploration](https://arxiv.org/abs/2404.12824)
* Authors: Shaohao Zhu, Jiacheng Zhou, Anjun Chen, Mingming Bai, Jiming Chen, Jinming Xu
* Reason: The paper introduces a versatile platform designed to address the sim-to-real gap in multi-agent reinforcement learning, providing what appears to be a significant contribution to efficient sampling and algorithmic diversity in various scenarios.

2. 8.6 [Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation](https://arxiv.org/abs/2404.12648)
* Authors: Jianliang He, Han Zhong, Zhuoran Yang
* Reason: This paper introduces a new comprehensive theoretical framework for AMDPs, suggesting a potentially high impact in the field of reinforcement learning with general function approximation.

3. 8.5 [Zero-Shot Stitching in Reinforcement Learning using Relative Representations](https://arxiv.org/abs/2404.12917)
* Authors: Antonio Pio Ricciardi, Valentino Maiorca, Luca Moschella, Riccardo Marin, Emanuele Rodol√†
* Reason: This work proposes a novel approach to visual reinforcement learning that allows for combining agent components to handle new scenarios, addressing the retraining issue caused by input variations, which can be influential for the development of more flexible and adaptable RL systems.

4. 8.4 [Continuous-time Risk-sensitive Reinforcement Learning via Quadratic Variation Penalty](https://arxiv.org/abs/2404.12598)
* Authors: Yanwei Jia
* Reason: Explores an innovative approach to continuous-time risk-sensitive reinforcement learning, penned by an authority who has advanced the field in related prior work.

5. 8.3 [Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2404.12999)
* Authors: Lisheng Wu, Ke Chen
* Reason: Introduces a framework that enhances exploration efficiency in goal-conditioned reinforcement learning by leveraging environmental structural patterns, promising improvements in the challenging domain of long-horizon and sparse rewards RL tasks.

6. 8.2 [Single-Task Continual Offline Reinforcement Learning](https://arxiv.org/abs/2404.12639)
* Authors: Sibo Gai, Donglin Wang
* Reason: Presents a novel approach to single-task offline RL with a continual learning perspective, addressing an emerging challenge in the domain.

7. 8.1 [Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models](https://arxiv.org/abs/2404.13013)
* Authors: Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, Xiaojuan Qi
* Reason: The paper presents Groma, an MLLM with visual perception capabilities and a localized visual tokenization mechanism that may significantly impact the realm of grounded language models and their practical applications.

8. 8.0 [TrajDeleter: Enabling Trajectory Forgetting in Offline Reinforcement Learning Agents](https://arxiv.org/abs/2404.12530)
* Authors: Chen Gong, Kecen Li, Jin Yao, Tianhao Wang
* Reason: Introduces the first practical approach for trajectory unlearning in offline RL, which could have significant implications for data privacy.

9. 7.9 [Analysis of Classifier-Free Guidance Weight Schedulers](https://arxiv.org/abs/2404.13040)
* Authors: Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, Vicky Kalogeiton
* Reason: Offers an analysis of weight schedulers in Classifier-Free Guidance, which is key to enhancing text-to-image diffusion models. Though the paper's focus is not purely on reinforcement learning, insights from this research could influence the generation of environment or scenario imaging for RL agents.

10. 7.8 [Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation](https://arxiv.org/abs/2404.12754)
* Authors: Qiang He, Tianyi Zhou, Meng Fang, Setareh Maghsudi
* Reason: Proposes a new regularization technique for controlling the representation rank in DRL, backed by empirical results showing performance improvements on continuous control tasks.

