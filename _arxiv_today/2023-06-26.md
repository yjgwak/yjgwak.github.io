---
title: Mon, 26 Jun 2023
date: 2023-06-26
---
1. 9.5 [TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning](https://arxiv.org/abs/2306.13229)
* Authors: Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daum√© III, Furong Huang
* The paper introduces a novel approach applying temporal contrastive learning for state and action representation learning in reinforcement learning, making significant impacts on both online and offline reinforcement learning scenarios.

2. 9.4 [Active Coverage for PAC Reinforcement Learning](https://arxiv.org/abs/2306.13601)
* Authors: Aymen Al-Marjani, Andrea Tirinzoni, Emilie Kaufmann
* Particularly intriguing is the notion of "active coverage" that allows online exploration-based learning. Through a game-theoretic algorithm, the authors allow for far more effective exploration of the environment in a reinforcement learning context.

3. 9.2 [Offline Skill Graph (OSG): A Framework for Learning and Planning using Offline Reinforcement Learning Skills](https://arxiv.org/abs/2306.13630)
* Authors: Ben-ya Halevy, Yehudit Aperstein, Dotan Di Castro
* This paper presents an innovative framework for planning over offline skills to solve complex tasks in real-world environments. Its potential to enhance the application of reinforcement learning in various practical scenarios gives it a potential for impact.

4. 8.9 [Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation](https://arxiv.org/abs/2306.13554)
* Authors: Massimiliano Patacchiola, Mingfei Sun, Katja Hofmann, Richard E. Turner
* The research interestingly provides insights on few-shot imitation learning, showing the effectiveness of a fine-tuning approach and potentially impacting future studies on control problems.

5. 8.7 [Correcting discount-factor mismatch in on-policy policy gradient methods](https://arxiv.org/abs/2306.13284)
* Authors: Fengdi Che, Gautham Vasan, A. Rupam Mahmood
* Reinforcement learning can benefit from the solution presented in this paper which corrects the discount factor, potentially ensuring more stable and efficient learning algorithms.

