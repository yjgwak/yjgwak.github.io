---
title: Fri, 9 Jun 2023
date: 2023-06-09
---
1. 9.5 [Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning](https://arxiv.org/abs/2306.04660)
* Authors: Ming Xu, Dongyu Zuo
* Reason: Presents a novel GLOSA model leveraging reinforcement learning to minimize traffic congestion and fuel consumption. Highly relevant to modern traffic control concerns and could influence research in ML applications to large-scale systems.

2. 9.3 [Unconstrained Online Learning with Unbounded Losses](https://arxiv.org/abs/2306.04923)
* Authors: Andrew Jacobsen, Ashok Cutkosky
* Reason: The paper explores the uncharted area of online learning with unbounded losses, extending the boundaries of machine learning research. Its authors are well-established in the field.

3. 9.2 [covLLM: Large Language Models for COVID-19 Biomedical Literature](https://arxiv.org/abs/2306.04926)
* Authors: Yousuf A. Khan, Clarisse Hokia, Jennifer Xu, Ben Ehlert
* Reason: The application of large language models to the problem of information extraction in COVID-19 research is a timely and potentially impactful contribution. Authors have influential backgrounds.

4. 9.0 [Learning to Navigate in Turbulent Flows with Aerial Robot Swarms: A Cooperative Deep Reinforcement Learning Approach](https://arxiv.org/abs/2306.04781)
* Authors: Diego Patiño, Siddharth Mayya, Juan Calderon, Kostas Daniilidis, David Saldaña
* Reason: Presents a novel multi-robot controller to navigate turbulent flows using RL. Particularly impactful for robotics and RL in challenging environments.

5. 9.0 [Robust Learning with Progressive Data Expansion Against Spurious Correlation](https://arxiv.org/abs/2306.04949)
* Authors: Yihe Deng, Yu Yang, Baharan Mirzasoleiman, Quanquan Gu
* Reason: This paper addresses the key issue of spurious correlations in deep learning models, offering robust solutions. The authors have significant contributions to ML.

6. 8.9 [A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels](https://arxiv.org/abs/2306.05046)
* Authors: Yifan Yang, Alec Koppel, Zheng Zhang
* Reason: This paper contributes to the important area of training ML models with noisy labels, particularly in an online learning setting. Authors have strong influence.

7. 8.9 [Decision S4: Efficient Sequence-Based RL via State Spaces Layers](https://arxiv.org/abs/2306.05167)
   * Authors: Shmuel Bar-David, Itamar Zimerman, Eliya Nachmani, Lior Wolf
   * This paper presents two algorithms for off-policy and on-policy training procedures with sequence learning methods. The presented approach reduces latency, number of parameters, and training time, making it suitable for real-world reinforcement learning.

8. 8.7 [$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control](https://arxiv.org/abs/2306.04836)
* Authors: Michael Giegrich, Roel Oomen, Christoph Reisinger
* Reason: A novel resampling procedure for estimating the off-policy performance, applicable in stochastic control environments. The theoretical backbone of the procedure and its applicability makes it highly influential.

9. 8.7 [Conformal Prediction for Federated Uncertainty Quantification Under Label Shift](https://arxiv.org/abs/2306.05131)
* Authors: Vincent Plassier, Mehdi Makni, Aleksandr Rubashevskii, Eric Moulines, Maxim Panov
* Reason: This paper tackles the emerging and challenging field of uncertainty quantification in federated learning under label shift, and it introduces a new conformal prediction method. Authors are recognized experts in statistical learning and ML.

10. 8.4 [Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning](https://arxiv.org/abs/2306.04875)
* Authors: Jifeng Hu, Yanchao Sun, Sili Huang, SiYuan Guo, Hechang Chen, Li Shen, Lichao Sun, Yi Chang, Dacheng Tao
* Reason: Presents an effective temporally-conditional diffusion model for better controlled generation in offline reinforcement learning. May shape future approaches in controlled generation in RL.

11. 8.4 [Offline Prioritized Experience Replay](https://arxiv.org/abs/2306.05412)
   * Authors: Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, Shuicheng Yan
   * This paper benefits the learning in offline reinforcement learning (RL) by the priority functions to prefer highly-rewarding transitions. It is able to improve the baseline methods with two practical strategies.

12. 8.2 [A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments](https://arxiv.org/abs/2306.05310)
   * Authors: Guangyao Zheng, Shuhao Lai, Vladimir Braverman, Michael A. Jacobs, Vishwa S. Parekh
   * This medical imaging paper implements lifelong reinforcement learning for adapting to changing imaging environments and adjust to low-compute devices.

13. 8.1 [Automatic retrieval of corresponding US views in longitudinal examinations](https://arxiv.org/abs/2306.04739)
* Authors: Hamideh Kerdegari, Tran Huy Nhat Phung1, Van Hao Nguyen, Thi Phuong Thao Truong, Ngoc Minh Thu Le, Thanh Phuong Le, Thi Mai Thao Le, Luigi Pisani, Linda Denehy, Vital Consortium, Reza Razavi, Louise Thwaites, Sophie Yacoub, Andrew P. King, Alberto Gomez
* Reason: Combines ML with medical imaging to improve patient outcomes. Relevant to reinforcement learning in healthcare. As it's not a pure reinforcement learning paper, it gets a lower score, but the predicted impact in its domain is high.

14. 7.8 [Federated Linear Contextual Bandits with User-level Differential Privacy](https://arxiv.org/abs/2306.05275)
   * Authors: Ruiquan Huang, Huanyu Zhang, Luca Melis, Milan Shen, Meisam Hajzinia, Jing Yang
   * This paper presents a federated bandits framework with user-level differential privacy. It provides insights on the learning regrets compared to the privacy budget.

15. 7.2 [Large-scale Dataset Pruning with Dynamic Uncertainty](https://arxiv.org/abs/2306.05175)
   * Authors: Muyang He, Shuo Yang, Tiejun Huang, Bo Zhao
   * It introduces a new method for large-scale dataset pruning, valuable particularly, for image classification tasks. Experimentally, it provides up to 75% compression with negligible reduction in performance.

