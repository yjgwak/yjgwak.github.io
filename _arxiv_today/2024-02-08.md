---
title: Thu, 8 Feb 2024
date: 2024-02-08
---
1. 9.7 [Latent Plan Transformer: Planning as Latent Variable Inference](https://arxiv.org/abs/2402.04647)
* Authors: Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew Lizarraga, Yuhao Huang, Sirui Xie, Ying Nian Wu
* Reason: This paper addresses the challenge of long-term planning in reinforcement learning through a novel generative modeling approach. The Latent Plan Transformer (LPT) bridges generation of trajectories and final return outcomes via a latent space. Its experimental performance across various benchmarks makes it potentially influential for future RL methods focusing on planning and decision-making.

2. 9.5 [Code as Reward: Empowering Reinforcement Learning with VLMs](https://arxiv.org/abs/2402.04764)
* Authors: David Venuto, Sami Nur Islam, Martin Klissarov, Doina Precup, Sherry Yang, Ankit Anand
* Reason: The paper presents a framework, VLM-CaR, that integrates the capabilities of pre-trained Vision-Language Models with RL agents in a novel way. Given the authorial authority of Doina Precup and the relevance of VLMs in contemporary RL research, this paper scores high on potential influence.

3. 9.3 [Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy](https://arxiv.org/abs/2402.04869)
* Authors: Ruichu Cai, Siyang Huang, Jie Qiao, Wei Chen, Yan Zeng, Keli Zhang, Fuchun Sun, Yang Yu, Zhifeng Hao
* Reason: This work's focus on integrating causal reasoning with RL and the proposal of a novel framework that learns from interventions could be a stepping stone towards more interpretable and efficient RL algorithms.

4. 9.2 [Grandmaster-Level Chess Without Search](https://arxiv.org/abs/2402.04494)
* Authors: Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein
* Reason: Training a large-scale transformer model on a dataset of 10 million chess games and outperforming established AI models like AlphaZero and GPT-3.5-turbo-instruct represents a significant advancement in machine learning, demonstrating the potential to reach high performance without domain-specific algorithms or explicit search methods.

5. 9.1 [Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning](https://arxiv.org/abs/2402.04894)
* Authors: Apoorva Vashisth, Julius Rückin, Federico Magistri, Cyrill Stachniss, Marija Popović
* Reason: By proposing a deep RL approach that adapts to 3D environments, this paper promises advancements in robotic path planning and target detection, with potential applicability in real-world scenarios such as orchard monitoring.

6. 8.9 [A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs](https://arxiv.org/abs/2402.04493)
* Authors: Kihyuk Hong, Ambuj Tewari
* Reason: Introducing a computationally efficient algorithm with improved sample complexity for offline RL is particularly influential as it addresses partial data coverage assumptions, which is a critical advancement in the field of reinforcement learning.

7. 8.9 [NITO: Neural Implicit Fields for Resolution-free Topology Optimization](https://arxiv.org/abs/2402.05073)
* Authors: Amin Heyrani Nobari, Giorgio Giannone, Lyle Regenwetter, Faez Ahmed
* Reason: The work introduces a novel approach that accelerates topology optimization using deep learning, offering resolution-free, domain-agnostic solutions. Its performance and efficiency make it potentially influential in engineering design optimization.

8. 8.7 [AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies](https://arxiv.org/abs/2402.04292)
* Authors: Xixi Hu, Bo Liu, Xingchao Liu, Qiang Liu
* Reason: Proposing AdaFlow, a novel and efficient imitation learning framework, is innovative and has the potential to benefit various multi-modal decision-making scenarios due to its ability to balance inference speed without sacrificing action diversity.

9. 8.5 [Incentivized Truthful Communication for Federated Bandits](https://arxiv.org/abs/2402.04485)
* Authors: Zhepei Wei, Chuanhao Li, Tianze Ren, Haifeng Xu, Hongning Wang
* Reason: This paper's focus on incentive-compatible communication protocols within federated bandit learning could have significant implications for the practicality and efficiency of federated learning, a subset of machine learning that is growing in importance with increased interest in privacy-preserving and distributed ML systems.

10. 8.3 [DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems](https://arxiv.org/abs/2402.04467)
* Authors: Yair Schiff, Zhong Yi Wan, Jeffrey B. Parker, Stephan Hoyer, Volodymyr Kuleshov, Fei Sha, Leonardo Zepeda-Núñez
* Reason: The paper presents a novel framework for learning dynamics from dissipative chaotic systems by targeting invariant measures—a concept that could lead to advancements in chaotic system modeling and is indicative of potential future influence in fields such as climate and weather modeling.

