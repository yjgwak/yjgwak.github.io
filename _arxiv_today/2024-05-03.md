---
title: Fri, 3 May 2024
date: 2024-05-03
---
1. 9.1 [Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks](https://arxiv.org/abs/2405.01534)
* Authors: Murtaza Dalal, Tarun Chiruvolu, Devendra Chaplot, Ruslan Salakhutdinov
* Reason: This paper was published at ICLR 2024, a highly reputable conference, which increases the credibility of the work. The authors propose a significant advancement in using Large Language Models for guiding RL policies, enabling solving complex long-horizon robotic tasks from scratch. The state-of-the-art results and contributions to multiple challenging benchmarks indicate substantial potential influence in the field of robotic reinforcement learning.

2. 9.0 [EEG_RL-Net: Enhancing EEG MI Classification through Reinforcement Learning-Optimised Graph Neural Networks](https://arxiv.org/abs/2405.00723)
* Authors: Htoo Wai Aung, Jiao Jiao Li, Yang An, Steven W. Su
* Reason: This paper presents a significant advancement in EEG MI signal classification with high accuracy, which is a substantial contribution given the importance of BCIs in medical and assistive technologies. The novel RL approach and exceptional results suggest a high impact potential, combined with the fact that Steven W. Su has authority in the field.

3. 8.9 [Constrained Reinforcement Learning Under Model Mismatch](https://arxiv.org/abs/2405.01327)
* Authors: Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou
* Reason: The paper addresses a critical issue of model mismatch between training and deployment environments in constrained RL, and offers an algorithm with theoretical guarantees. This work is likely to influence future research in robust RL applied to real-world environments.

4. 8.8 [MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure](https://arxiv.org/abs/2405.00902)
* Authors: Zhicheng Zhang, Yancheng Liang, Yi Wu, Fei Fang
* Reason: Accepted in AAMAS 2024, demonstrating peer validation of the work's novelty. The paper addresses efficient exploration in MARL which is critical in achieving near Pareto optimal strategies and could influence future research in sparse-reward multi-agent environments.

5. 8.7 [Learning Force Control for Legged Manipulation](https://arxiv.org/abs/2405.01402)
* Authors: Tifanny Portela, Gabriel B. Margolis, Yandong Ji, Pulkit Agrawal
* Reason: Accepted to ICRA24 and targets the critical aspect of force control in legged robotics, an essential topic for advancing robotics manipulation. The novel methodology and the application to real-world problems suggest potential influence, particularly in the field of robotics and control systems.

6. 8.6 [Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback](https://arxiv.org/abs/2405.00950)
* Authors: Guojun Xiong, Jian Li
* Reason: Acceptance by ICML 2024 indicates high peer regard, and the work addresses a critical and challenging aspect of learning in RMAB with unknown transitions and adversarial rewards, suggesting a significant step forward in the domain. The contribution seems fundamentally influential for future work in RMAB settings.

7. 8.5 [Towards Interpretable Reinforcement Learning with Constrained Normalizing Flow Policies](https://arxiv.org/abs/2405.01198)
* Authors: Finn Rietz, Erik Schaffernicht, Stefan Heinrich, Johannes A. Stork
* Reason: This paper proposes an interpretable and safe-by-construction policy model which is crucial for deploying RL in safety-critical domains. The focus on interpretability and safety may impact a wide range of future RL applications, though the influence may be dependent on adoption and further empirical results.

8. 8.4 [Gameplay Filters: Safe Robot Walking through Adversarial Imagination](https://arxiv.org/abs/2405.00846)
* Authors: Duy P. Nguyen, Kai-Chieh Hsu, Wenhao Yu, Jie Tan, Jaime F. Fisac
* Reason: Ensuring robot operational safety is of great importance, and this paper's novel use of offline game-theoretic RL for this purpose is both innovative and practically relevant, especially with demonstrated effectiveness in physical experiments.

9. 8.3 [A Review of Reward Functions for Reinforcement Learning in the context of Autonomous Driving](https://arxiv.org/abs/2405.01440)
* Authors: Ahmed Abouelazm, Jonas Michel, J. Marius Zoellner
* Reason: As a substantial review on reward functions specifically for autonomous driving, it may influence future designs of RL algorithms in this domain. It identifies gaps and proposes future research directions, potentially guiding subsequent efforts to standardize reward formulations in autonomous vehicle systems.

10. 8.2 [LOQA: Learning with Opponent Q-Learning Awareness](https://arxiv.org/abs/2405.01035)
* Authors: Milad Aghajohari, Juan Agustin Duque, Tim Cooijmans, Aaron Courville
* Reason: The paper presents LOQA, a new algorithm for multi-agent learning, a topic with wide applicability. The inclusion of a notable author, Aaron Courville, and the potential application in general-sum games in real-world scenarios underlines the paperâ€™s possible influence.

