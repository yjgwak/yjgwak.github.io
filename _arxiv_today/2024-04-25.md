---
title: Thu, 25 Apr 2024
date: 2024-04-25
---
1. 9.2 [The Power of Resets in Online Reinforcement Learning](https://arxiv.org/abs/2404.15417)
* Authors: Zakaria Mhammedi, Dylan J. Foster, Alexander Rakhlin
* Reason: Breakthrough in utilizing simulators with local planning, providing new statistical guarantees and a more efficient algorithm, RVFS, which improves upon real-world RL applications.

2. 9.0 [GRSN: Gated Recurrent Spiking Neurons for POMDPs and MARL](https://arxiv.org/abs/2404.15597)
* Authors: Lang Qin, Ziming Wang, Runhao Jiang, Rui Yan, Huajin Tang
* Reason: Novel approach in aligning SNNs with RL temporal demands, showing promise in POMDPs and MARL domains while maintaining computational efficiency.

3. 8.9 [DPO: Differential reinforcement learning with application to optimal configuration search](https://arxiv.org/abs/2404.15617)
* Authors: Chandrajit Bajaj, Minh Nguyen
* Reason: Novel framework of Differential Policy Optimization promising efficient handling of continuous spaces in RL, with applications in complex problem-solving and configuration search.

4. 8.9 [Recursive Backwards Q-Learning in Deterministic Environments](https://arxiv.org/abs/2404.15822)
* Authors: Jan Diekhoff, Jörn Fischer
* Reason: Introduces a novel reinforcement learning algorithm aimed at solving deterministic problems more effectively, which is a direct contribution to reinforcement learning advancements.

5. 8.7 [ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos](https://arxiv.org/abs/2404.15709)
* Authors: Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev
* Reason: Offers a practical approach to learning from human videos for robot manipulation, handling limitations of previous methods, and improving upon the ability of robots to perform dexterous tasks.

6. 8.7 [An Element-Wise Weights Aggregation Method for Federated Learning](https://arxiv.org/abs/2404.15919)
* Authors: Yi Hu, Hanchi Ren, Chen Hu, Jingjing Deng, Xianghua Xie
* Reason: The paper focuses on a central challenge in federated learning, which is closely related to reinforcement learning in terms of decentralized learning processes and possible significant improvement on learning performance.

7. 8.6 [FedSI: Federated Subnetwork Inference for Efficient Uncertainty Quantification](https://arxiv.org/abs/2404.15657)
* Authors: Hui Chen, Hengyu Liu, Zhangkai Wu, Xuhui Fan, Longbing Cao
* Reason: Introduces an innovative approach to personalize federated learning, addressing uncertainty and systematizing efficiency across distributed networks.

8. 8.3 [ST-MambaSync: The Confluence of Mamba Structure and Spatio-Temporal Transformers for Precipitous Traffic Prediction](https://arxiv.org/abs/2404.15899)
* Authors: Zhiqi Shao, Xusheng Yao, Ze Wang, Junbin Gao
* Reason: It introduces an innovative framework potentially beneficial for real-time scenarios in reinforcement learning universe, specifically for applications considering spatial-temporal prediction tasks.

9. 8.1 [Explainable AI models for predicting liquefaction-induced lateral spreading](https://arxiv.org/abs/2404.15959)
* Authors: Cheng-Hsi Hsiao, Krishna Kumar, Ellen Rathje
* Reason: While not directly linked to reinforcement learning, the use of explainable AI is critical to understanding model decisions, which could have implications on reinforcement learning approaches that need interpretability.

10. 7.9 [Fast Ensembling with Diffusion Schrödinger Bridge](https://arxiv.org/abs/2404.15814)
* Authors: Hyunsu Kim, Jongmin Yoon, Juho Lee
* Reason: Proposes a novel ensembling technique that can be used to enhance performance of deep neural networks, relevant for reinforcement learning contexts where ensemble learning may offer improvements.

