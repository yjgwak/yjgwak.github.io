---
title: Fri, 16 Jun 2023
date: 2023-06-16
---
1. 9.9 [Multi-market Energy Optimization with Renewables via Reinforcement Learning](https://arxiv.org/abs/2306.08147)
* Authors: Lucien Werner, Peeyush Kumar
* Reason: This paper introduces a deep reinforcement learning (RL) framework for energy market maximization and offers a novel method to ensure policy actions respect system constraints. Therefore, it holds high significance in real-world applications of RL in the energy sector. The authors are also established in the field.

2. 9.7 [Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning](https://arxiv.org/abs/2306.08094)
* Authors: Michael Villarreal, Bibek Poudel, Weizi Li
* Reason: The study is a large-scale user study, and it investigates how novices can leverage ChatGPT (an advanced language model) to solve complex mixed traffic control problems using reinforcement learning models.

3. 9.6 [Curricular Subgoals for Inverse Reinforcement Learning](https://arxiv.org/abs/2306.08232)
* Authors: Shunyu Liu, Yunpeng Qing, Shuqi Xu, Hongyan Wu, Jiangtao Zhang, Jingyuan Cong, Tianhao Chen, Yunfu Liu, Mingli Song
* Reason: This paper introduces a novel Curricular Subgoal-based IRL framework, that disentangles complex tasks with several local subgoals to guide agent imitation. The proposed method shows superior results and better interpretability.

4. 9.5 [Dynamic Interval Restrictions on Action Spaces in Deep Reinforcement Learning for Obstacle Avoidance](https://arxiv.org/abs/2306.08008)
* Authors: Tim Grams
* Reason: This research proposes and achieves effective methods regarding interval restrictions in deep reinforcement learning for obstacle avoidance, which could be applicable to a range of real-world applications.

5. 9.5 [Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources](https://arxiv.org/abs/2306.08364)
* Authors: Chengshuai Shi, Wei Xiong, Cong Shen, Jing Yang
* Reason: This paper introduces a unique approach to offline reinforcement learning (RL), considering the practical scenario where data comes from heterogeneous but related sources, rather than directly from the target task. It not only introduces a novel algorithm called HetPEVI, but also extends the study to offline Markov games and offline robust RL, demonstrating the generality of the proposed designs and theoretical analyses.

6. 9.3 [A Markovian Formalism for Active Querying](https://arxiv.org/abs/2306.08001)
* Authors: Sid Ijju
* Reason: Even though the author isn't a well-established figure in the field, the paper outlines an original and relevant Markovian formalism for active learning, which can potentially influence and streamline research in fields of AI and machine learning.

7. 9.2 [Skill-Critic: Refining Learned Skills for Reinforcement Learning](https://arxiv.org/abs/2306.08388)
* Authors: Ce Hao, Catherine Weaver, Chen Tang, Kenta Kawamoto, Masayoshi Tomizuka, Wei Zhan
* Reason: The paper focuses on the improving the reliability of low-level policies in hierarchical RL, using a novel algorithm called Skill-Critic. It involves the optimization of low and high-level policies, with the policies additionally regularized by the latent space learned from offline demonstrations. The approach was validated in diverse RL environments, showing the effectiveness of the proposed method.

8. 9.0 [Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement Learning](https://arxiv.org/abs/2306.08400)
* Authors: Evan Zheran Liu, Sahaana Suri, Tong Mu, Allan Zhou, Chelsea Finn
* Reason: This paper poses and explores an interesting question about the potential for RL agents themselves to indirectly learn language through non-language tasks. The investigation was conducted in a software-based environment, with the aim of optimizing agents to read and interpret visually rendered floor plans.

9. 9.0 [Inroads into Autonomous Network Defence using Explained Reinforcement Learning](https://arxiv.org/abs/2306.09318)
* Authors: Myles Foley, Mia Wang, Zoe M, Chris Hicks, Vasilios Mavroudis
* Reason: The work is addressed towards network security and has been provided with a comprehensive methodology from understanding attack strategies to designing defence models and explaining them. The authors achieved a significant performance improvement which demonstrates the effectiveness and potential influential impact of their work.

10. 8.8 [Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization](https://arxiv.org/abs/2306.08900)
* Authors: Xiangsen Wang, Xianyuan Zhan
* Reason: The authors delve into offline multi-agent reinforcement learning and propose a new algorithm that drives a better performance in complex tasks. The use case of StarCraft II micro-management tasks and the cited superior performance over existing offline multi-agent RL methods suggest a potential influential impact.

11. 8.7 [Mediated Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2306.08419)
* Authors: Dmitry Ivanov, Ilya Zisman, Kirill Chernyshev
* Reason: This paper presents a novel approach to Multi-Agent RL, putting forward the concept of cooperation as finding socially beneficial equilibrium, and proposing a mediation framework that can maximize social welfare while encouraging agent cooperation. The validity and potential benefits of this approach are demonstrated through experimental results.

12. 8.5 [VIBR: Learning View-Invariant Value Functions for Robust Visual Control](https://arxiv.org/abs/2306.08537)
* Authors: Tom Dupuis, Jaonary Rabarisoa, Quoc-Cuong Pham, David Filliat
* Reason: This paper focuses on improving RL performance in visually diverse environments through a technique called VIBR, which utilises multi-view training and invariant prediction to reduce the generalization gap for RL-based visuomotor control. The method improves baseline performances and shows significant proficiency in robust control across a variety of experimental environments.

13. 8.5 [Semantic HELM: An Interpretable Memory for Reinforcement Learning](https://arxiv.org/abs/2306.09312)
* Authors: Fabian Paischer, Thomas Adler, Markus Hofmarcher, Sepp Hochreiter
* Reason: Notably focusing on the interpretability of reinforcement learning, the authors propose a novel, language-based memory mechanism. The work has wide implications given the importance of interpretability in real-world applications of RL, and their state-of-the-art performance in environments with crucial memory-related tasks.

14. 8.3 [DiAReL: Reinforcement Learning with Disturbance Awareness for Robust Sim2Real Policy Transfer in Robot Control](https://arxiv.org/abs/2306.09010)
* Authors: Mohammadhossein Malmir, Josip Josifovski, Noah Klarmann, Alois Knoll
* Reason: Addressing robustness in reinforcement learning, the authors propose a novel disturbance-augmented Markov decision approach in robotic control. They show higher stabilization and robustness in the control response which benefits Sim2Real transfer, contributing positively to the area.

15. 8.1 [Optimal Exploration for Model-Based RL in Nonlinear Systems](https://arxiv.org/abs/2306.09210)
* Authors: Andrew Wagenmaker, Guanya Shi, Kevin Jamieson
* Reason: This paper deals with control of unknown nonlinear dynamical systems and proposes an algorithm to efficiently explore the system, which brings a significant contribution to the RL in nonlinear systems. The authors also report the effectiveness of their method in realistic nonlinear robotic systems.

