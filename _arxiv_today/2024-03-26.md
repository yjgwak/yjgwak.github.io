---
title: Tue, 26 Mar 2024
date: 2024-03-26
---
1. 9.3 [Physics-informed RL for Maximal Safety Probability Estimation](https://arxiv.org/abs/2403.16391)
* Authors: Hikaru Hoshino, Yorie Nakahira
* Reason: The paper introduces a novel reinforcement learning method integrating physics to improve safety in control systems, which is highly impactful for real-world applications. The methodology of learning with sparse rewards addresses a crucial limitation in current deep RL methods.

2. 9.0 [Bridging the Sim-to-Real Gap with Bayesian Inference](https://arxiv.org/abs/2403.16644)
* Authors: Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
* Reason: Presents SIM-FSVGD, a technique that leverages simulators for training neural network models, which is critical for model-based RL tasks, and demonstrates a significant reduction in data requirements for learning robot dynamics.

3. 8.9 [A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services](https://arxiv.org/abs/2403.15780)
* Authors: Luca Vittorio Piron, Matteo Cederle, Marina Ceccon, Federico Chiariotti, Alessandro Fabris, Marco Fabris, Gian Antonio Susto
* Reason: Introduces a novel fairness-oriented approach to reinforcement learning in micromobility systems, balancing performance optimization with equity, and uses established Q-Learning algorithm with Gini index for fairness measurement. Significance due to its application in urban transportation and emphasis on fairness.

4. 8.8 [Scaling Learning based Policy Optimization for Temporal Tasks via Dropout](https://arxiv.org/abs/2403.15826)
* Authors: Navid Hashemi, Bardh Hoxha, Danil Prokhorov, Georgios Fainekos, Jyotirmoy Deshmukh
* Reason: Addresses long-horizon tasks in reinforcement learning with a novel gradient approximation algorithm, potentially influencing learning in nonlinear environments and temporal logic with computational advantages.

5. 8.7 [Producing and Leveraging Online Map Uncertainty in Trajectory Prediction](https://arxiv.org/abs/2403.16439)
* Authors: Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic
* Reason: Addresses a real-world problem in the domain of autonomous vehicles and improves the integration of mapping with trajectory forecasting by incorporating uncertainty, which could greatly influence future work in this area.

6. 8.6 [Initialisation and Topology Effects in Decentralised Federated Learning](https://arxiv.org/abs/2403.15855)
* Authors: Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, János Kertész, Márton Karsai
* Reason: Sheds light on the significance of network topology in the performance of decentralized federated learning, with an innovative neural network initialisation strategy that might set groundwork for future distributed learning dynamics.

7. 8.6 [Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics](https://arxiv.org/abs/2403.16680)
* Authors: Rene Winchenbach, Nils Thuerey
* Reason: Targets the application of machine learning in physical simulations, offering a novel method that could significantly outperform established architectures in terms of accuracy and generalization.

8. 8.4 [Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time](https://arxiv.org/abs/2403.15928)
* Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu
* Reason: Proposes an online reinforcement learning algorithm that ensures safety without violating constraints during learning in stochastic environments, a key feature for many practical applications that involve high-risk decision-making.

9. 8.4 [Weak Convergence Analysis of Online Neural Actor-Critic Algorithms](https://arxiv.org/abs/2403.16825)
* Authors: Samuel Chun-Hei Lam, Justin Sirignano, Ziheng Wang
* Reason: Provides a rigorous theoretical foundation for the convergence of neural actor-critic algorithms, a fundamental aspect of RL, which could have broad implications for the stability of RL training methods.

10. 8.2 [Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search](https://arxiv.org/abs/2403.15908)
* Authors: Can Bogoclu, Robert Vosshall, Kevin Cremanns, Dirk Roos
* Reason: Enhances data efficiency in model-based reinforcement learning by integrating deep Gaussian covariance networks and trajectory sampling, which could significantly improve computational performance and robustness in optimal control problems.

